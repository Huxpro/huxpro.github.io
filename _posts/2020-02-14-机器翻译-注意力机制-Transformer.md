---
layout:     post
title:      PyTorchç³»åˆ—ä¹‹æœºå™¨ç¿»è¯‘ï½œæ³¨æ„åŠ›æœºåˆ¶ï½œTransformer
subtitle:   æœºå™¨ç¿»è¯‘ï½œæ³¨æ„åŠ›æœºåˆ¶ï½œTransformer
date:       2020-02-14
author:     Young
header-img: img/1*CNzKOooCM4hQgK4Z89sQLQ.png
catalog: true
tags:
    - PyTorch
---

## Task04ï¼šæœºå™¨ç¿»è¯‘åŠç›¸å…³æŠ€æœ¯ï½œæ³¨æ„åŠ›æœºåˆ¶ä¸Seq2seqæ¨¡å‹ï½œTransformer

### æœºå™¨ç¿»è¯‘

- **å›°éš¾ä¹‹å¤„**ï¼šè¾“å‡ºåºåˆ—çš„é•¿åº¦å¯èƒ½ä¸æºåºåˆ—çš„é•¿åº¦ä¸åŒ

- **padding**ï¼šæ¯ä¸ªbatchçš„å¥å­é•¿åº¦ä¸ä¸€æ ·ï¼Œæ‰€ä»¥éœ€è¦paddingï¼Œpaddingåå¥å­é•¿åº¦ä¿æŒä¸€è‡´

  ```python
  def pad(line, max_len, padding_token):
      if len(line) > max_len:
          return line[:max_len]
      return line + [padding_token] * (max_len - len(line))
  ```

- **Sequence to Sequenceæ¨¡å‹**

  The sequence to sequence (seq2seq) model is based on the **encoder-decoder architecture** to **generate a sequence output for a sequence input**. Both the **encoder and the decoder use recurrent neural networks (RNNs) to handle sequence inputs of variable length**. The hidden state of the encoder is used directly to initialize the decoder hidden state to pass information from the encoder to the decoder.
  
  - **Train** 
  
  <p align="center">
    <img src="https://d2l.ai/_images/seq2seq.svg" style="zoom:100%" />
  </p>
  
  - **Predict** 
  
    <p align="center">
      <img src="https://d2l.ai/_images/seq2seq_predict.svg" style="zoom:100%" />
    </p>
  
    -  é¢„æµ‹æ—¶decoderæ¯ä¸ªå•å…ƒè¾“å‡ºå¾—åˆ°çš„å•è¯ä½œä¸ºä¸‹ä¸€ä¸ªå•å…ƒçš„è¾“å…¥å•è¯
    -  é¢„æµ‹æ—¶decoderå•å…ƒè¾“å‡ºä¸ºå¥å­ç»“æŸç¬¦æ—¶è·³å‡ºå¾ªç¯
  
  - **Layers**
  
    <p align="center">
      <img src="https://d2l.ai/_images/seq2seq-details.svg" style="zoom:100%" />
    </p>
  
    ```
    class Seq2SeqEncoder(d2l.Encoder):
        def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                     dropout=0, **kwargs):
            super(Seq2SeqEncoder, self).__init__(**kwargs)
            self.num_hiddens=num_hiddens
            self.num_layers=num_layers
            self.embedding = nn.Embedding(vocab_size, embed_size)
            self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)
       
        def begin_state(self, batch_size, device):
            return [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device),torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device)]
            
        def forward(self, X, *args):
            X = self.embedding(X) # X shape: (batch_size, seq_len, embed_size)
            X = X.transpose(0, 1)  # RNN needs first axes to be time
            # state = self.begin_state(X.shape[1], device=X.device)
            out, state = self.rnn(X)
            # The shape of out is (seq_len, batch_size, num_hiddens).
            # state contains the hidden state and the memory cell
            # of the last time step, the shape is (num_layers, batch_size, num_hiddens)
            return out, state
    ```
  
    ```
    class Seq2SeqDecoder(d2l.Decoder):
        def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                     dropout=0, **kwargs):
            super(Seq2SeqDecoder, self).__init__(**kwargs)
            self.embedding = nn.Embedding(vocab_size, embed_size)
            self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)
            self.dense = nn.Linear(num_hiddens,vocab_size)
    
        def init_state(self, enc_outputs, *args):
            return enc_outputs[1]
    
        def forward(self, X, state):
            X = self.embedding(X).transpose(0, 1)
            out, state = self.rnn(X, state)
            # Make the batch to be the first dimension to simplify loss computation.
            out = self.dense(out).transpose(0, 1)
            return out, state
    ```
  
- **<font color=red>Softmaxå±è”½ï½œæŸå¤±å‡½æ•°</font>** ï¼šåªç”¨æœ‰æ•ˆé•¿åº¦

  ```
  def SequenceMask(X, X_len,value=0):
  		"""
  		X_lenä¸ºæœ‰æ•ˆåˆ—æ•°ï¼Œå¯¹Xä¸­æ¯è¡Œæ•°æ®ï¼Œè¶…å‡ºå¯¹åº”æœ‰æ•ˆåˆ—æ•°çš„éƒ¨åˆ†èµ‹å€¼ä¸ºvalue
  		
  		>>> X = torch.tensor([[1,2,3], [4,5,6]])
  				SequenceMask(X,torch.tensor([1,2]))
  		>>> tensor([[1, 0, 0],
          				[4, 5, 0]])
  		"""
      maxlen = X.size(1)
      mask = torch.arange(maxlen)[None, :].to(X_len.device) < X_len[:, None]   
      X[~mask]=value
      return X
    
  
  class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):
      # pred shape: (batch_size, seq_len, vocab_size)
      # label shape: (batch_size, seq_len)
      # valid_length shape: (batch_size, )
      def forward(self, pred, label, valid_length):
          # the sample weights shape should be (batch_size, seq_len)
          weights = torch.ones_like(label)
          weights = SequenceMask(weights, valid_length).float()
          self.reduction='none'
          output=super(MaskedSoftmaxCELoss, self).forward(pred.transpose(1,2), label)
          return (output*weights).mean(dim=1)
  ```
  
- Beam Search è¦è§£å†³çš„é—®é¢˜

  - è´ªå¿ƒç®—æ³•ï¼šåªè€ƒè™‘å½“å‰æ—¶åˆ»çš„å±€éƒ¨æœ€ä¼˜è§£ï¼Œæ²¡æœ‰è€ƒè™‘å‰åè¯­ä¹‰æ˜¯å¦è¿è´¯ï¼ˆéå…¨å±€æœ€ä¼˜è§£ï¼‰

  - é›†æŸæœç´¢ä½¿ç”¨beam sizeå‚æ•°æ¥é™åˆ¶åœ¨æ¯ä¸€æ­¥ä¿ç•™ä¸‹æ¥çš„å¯èƒ½æ€§è¯çš„æ•°é‡

  - é›†æŸæœç´¢ç»“åˆäº†greedy searchå’Œç»´ç‰¹æ¯”ç®—æ³•

    <p align="center">
      <img src="https://d2l.ai/_images/beam-search.svg" style="zoom:100%" />
    </p>

### pythonçŸ¥è¯†ç‚¹

- **<font color=red>é­”æœ¯æ–¹æ³•</font>** ä¹‹`__getitem__`

  - **å½“å®ä¾‹å¯¹è±¡é€šè¿‡[] è¿ç®—ç¬¦å–å€¼æ—¶ï¼Œä¼šè°ƒç”¨å®ƒçš„æ–¹æ³•`__getitem__`**

  - å‡¡æ˜¯åœ¨ç±»ä¸­å®šä¹‰äº†è¿™ä¸ª`__getitem__` æ–¹æ³•ï¼Œé‚£ä¹ˆå®ƒçš„å®ä¾‹å¯¹è±¡ï¼ˆå‡å®šä¸ºpï¼‰ï¼Œå¯ä»¥åƒè¿™æ ·

    p[key] å–å€¼ï¼Œå½“å®ä¾‹å¯¹è±¡åšp[key] è¿ç®—æ—¶ï¼Œä¼šè°ƒç”¨ç±»ä¸­çš„æ–¹æ³•`__getitem__`ã€‚

    **ä¸€èˆ¬å¦‚æœæƒ³ä½¿ç”¨ç´¢å¼•è®¿é—®å…ƒç´ æ—¶ï¼Œå°±å¯ä»¥åœ¨ç±»ä¸­å®šä¹‰è¿™ä¸ªæ–¹æ³•`ï¼ˆ__getitem__(self, key) ï¼‰`ã€‚**

- **<font color=red>Pythonç¼–ç¨‹ä¸­NotImplementedErrorçš„ä½¿ç”¨</font>** 

  - Pythonç¼–ç¨‹ä¸­raiseå¯ä»¥å®ç°æŠ¥å‡ºé”™è¯¯çš„åŠŸèƒ½ï¼Œè€ŒæŠ¥é”™çš„æ¡ä»¶å¯ä»¥ç”±ç¨‹åºå‘˜è‡ªå·±å»å®šåˆ¶ã€‚**åœ¨é¢å‘å¯¹è±¡ç¼–ç¨‹ä¸­ï¼Œå¯ä»¥å…ˆé¢„ç•™ä¸€ä¸ªæ–¹æ³•æ¥å£ä¸å®ç°ï¼Œåœ¨å…¶å­ç±»ä¸­å®ç°ã€‚å¦‚æœè¦æ±‚å…¶å­ç±»ä¸€å®šè¦å®ç°ï¼Œä¸å®ç°çš„æ—¶å€™ä¼šå¯¼è‡´é—®é¢˜ï¼Œé‚£ä¹ˆé‡‡ç”¨raiseçš„æ–¹å¼å°±å¾ˆå¥½ã€‚è€Œæ­¤æ—¶äº§ç”Ÿçš„é—®é¢˜åˆ†ç±»æ˜¯NotImplementedError**ã€‚

  - **Encoder-Decoderä¸­çš„åº”ç”¨**

    <p align="center">
      <img src="https://d2l.ai/_images/encoder-decoder.svg" style="zoom:100%" />
    </p>

    - The encoder is a normal neural network that takes inputs, e.g., a source sentence, to return outputs.

      ```python
      class Encoder(nn.Module):
          def __init__(self, **kwargs):
              super(Encoder, self).__init__(**kwargs)
      
          def forward(self, X, *args):
              raise NotImplementedError
      ```

    - The decoder has an additional method `init_state` to parse the outputs of the encoder with possible additional information, e.g., the valid lengths of inputs, to return the state it needs. In the forward method, the decoder takes both inputs, e.g., a target sentence and the state. It returns outputs, with potentially modified state if the encoder contains RNN layers.

      ```python
      class Decoder(nn.Module):
          def __init__(self, **kwargs):
              super(Decoder, self).__init__(**kwargs)
      
          def init_state(self, enc_outputs, *args):
              raise NotImplementedError
      
          def forward(self, X, state):
              raise NotImplementedError
      ```

    - The encoder-decoder model contains both an encoder and a decoder. We implement its forward method for training. It takes both encoder inputs and decoder inputs, with optional additional arguments. During computation, it first computes encoder outputs to initialize the decoder state, and then returns the decoder outputs.

      ```python
      class EncoderDecoder(nn.Module):
          def __init__(self, encoder, decoder, **kwargs):
              super(EncoderDecoder, self).__init__(**kwargs)
              self.encoder = encoder
              self.decoder = decoder
      
          def forward(self, enc_X, dec_X, *args):
              enc_outputs = self.encoder(enc_X, *args)
              dec_state = self.decoder.init_state(enc_outputs, *args)
              return self.decoder(dec_X, dec_state)
      ```

      

### æ³¨æ„åŠ›æœºåˆ¶

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/vjKtq1.jpg" style="zoom:80%" />
</p>

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/E2QWyP.jpg" style="zoom:80%" />
</p>

- èƒŒæ™¯

  - åœ¨â€œç¼–ç å™¨â€”è§£ç å™¨ï¼ˆseq2seqï¼‰â€â¼€èŠ‚â¾¥ï¼Œè§£ç å™¨åœ¨å„ä¸ªæ—¶é—´æ­¥ä¾èµ–ç›¸åŒçš„èƒŒæ™¯å˜é‡ï¼ˆcontext vectorï¼‰æ¥è·å–è¾“â¼Šåºåˆ—ä¿¡æ¯ã€‚**å½“ç¼–ç å™¨ä¸ºå¾ªç¯ç¥ç»â½¹ç»œæ—¶ï¼ŒèƒŒæ™¯å˜é‡æ¥â¾ƒå®ƒæœ€ç»ˆæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ã€‚å°†æºåºåˆ—è¾“å…¥ä¿¡æ¯ä»¥å¾ªç¯å•ä½çŠ¶æ€ç¼–ç ï¼Œç„¶åå°†å…¶ä¼ é€’ç»™è§£ç å™¨ä»¥ç”Ÿæˆç›®æ ‡åºåˆ—**ã€‚ç„¶è€Œè¿™ç§ç»“æ„å­˜åœ¨ç€é—®é¢˜ï¼Œå°¤å…¶æ˜¯**RNNæœºåˆ¶å®é™…ä¸­å­˜åœ¨é•¿ç¨‹æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜**ï¼Œå¯¹äºè¾ƒé•¿çš„å¥å­ï¼Œæˆ‘ä»¬å¾ˆéš¾å¯„å¸Œæœ›äºå°†è¾“å…¥çš„åºåˆ—è½¬åŒ–ä¸ºå®šé•¿çš„å‘é‡è€Œä¿å­˜æ‰€æœ‰çš„æœ‰æ•ˆä¿¡æ¯ï¼Œæ‰€ä»¥éšç€æ‰€éœ€ç¿»è¯‘å¥å­çš„é•¿åº¦çš„å¢åŠ ï¼Œè¿™ç§ç»“æ„çš„æ•ˆæœä¼šæ˜¾è‘—ä¸‹é™ã€‚

    ä¸æ­¤åŒæ—¶ï¼Œè§£ç çš„ç›®æ ‡è¯è¯­å¯èƒ½åªä¸åŸè¾“å…¥çš„éƒ¨åˆ†è¯è¯­æœ‰å…³ï¼Œè€Œå¹¶ä¸æ˜¯ä¸æ‰€æœ‰çš„è¾“å…¥æœ‰å…³ã€‚ä¾‹å¦‚ï¼Œå½“æŠŠâ€œHello worldâ€ç¿»è¯‘æˆâ€œBonjour le mondeâ€æ—¶ï¼Œâ€œHelloâ€æ˜ å°„æˆâ€œBonjourâ€ï¼Œâ€œworldâ€æ˜ å°„æˆâ€œmondeâ€ã€‚**åœ¨seq2seqæ¨¡å‹ä¸­ï¼Œè§£ç å™¨åªèƒ½éšå¼åœ°ä»ç¼–ç å™¨çš„æœ€ç»ˆçŠ¶æ€ä¸­é€‰æ‹©ç›¸åº”çš„ä¿¡æ¯ã€‚ç„¶è€Œï¼Œæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥å°†è¿™ç§é€‰æ‹©è¿‡ç¨‹æ˜¾å¼åœ°å»ºæ¨¡ã€‚**

    <p align="center">
      <img src="https://d2l.ai/_images/attention.svg" style="zoom:100%" />
    </p>

    

- **æ³¨æ„åŠ›æœºåˆ¶æ¡†æ¶**

  - Attention æ˜¯ä¸€ç§é€šç”¨çš„å¸¦æƒæ± åŒ–æ–¹æ³•ï¼Œ**è¾“å…¥ç”±ä¸¤éƒ¨åˆ†æ„æˆï¼šè¯¢é—®ï¼ˆqueryï¼‰å’Œé”®å€¼å¯¹ï¼ˆkey-value pairsï¼‰ã€‚**

  - **å®Œæ•´è¿‡ç¨‹**ï¼š

    - first use **score function $\alpha$** that measures the similarity between the query and key $a_{i}=\alpha\left(\mathbf{q}, \mathbf{k}_{i}\right)$
    
    - Next we use **softmax** to obtain the attention weights 
    
      $\mathbf{b}=softmax(\mathbf{a}) \quad,$ where $\quad b_{i}=\frac{\exp \left(a_{i}\right)}{\sum_{j} \exp \left(a_{j}\right)}, \mathbf{b}=\left[b_{1}, \ldots, b_{n}\right]^{T}$
    
    - Finally, the output is a **weighted sum of the values**:
    
      $\mathbf{o}=\sum_{i=1}^{n} b_{i} \mathbf{v}_{i}$
    
    <p align="center">
      <img src="https://d2l.ai/_images/attention_output.svg" style="zoom:100%" />
    </p>
    
  - **ä¸åŒçš„attetion layerçš„åŒºåˆ«åœ¨äºscoreå‡½æ•°çš„é€‰æ‹©**

- **ä¸¤ä¸ªå¸¸ç”¨çš„æ³¨æ„å±‚ Dot-product Attention å’Œ Multilayer Perceptron Attention**

  - **ç‚¹ç§¯æ³¨æ„åŠ› Dot-product Attention**
  - The dot product å‡è®¾queryå’Œkeysæœ‰ç›¸åŒçš„ç»´åº¦, å³ $\forall i, ğª,ğ¤_ğ‘– âˆˆ â„_ğ‘‘ $. é€šè¿‡è®¡ç®—queryå’Œkeyè½¬ç½®çš„ä¹˜ç§¯æ¥è®¡ç®—attention score,é€šå¸¸è¿˜ä¼šé™¤å» $\sqrt{d}$ å‡å°‘è®¡ç®—å‡ºæ¥çš„scoreå¯¹ç»´åº¦ğ‘‘çš„ä¾èµ–æ€§ï¼Œå¦‚ä¸‹ 
    
    <p align="center">
    $$
    ğ›¼(ğª,ğ¤)=âŸ¨ğª,ğ¤âŸ©/ \sqrt{d}
    $$
    </p>
    
    å‡è®¾ $ ğâˆˆâ„^{ğ‘šÃ—ğ‘‘}$ æœ‰ $m$ ä¸ªqueryï¼Œ$ğŠâˆˆâ„^{ğ‘›Ã—ğ‘‘}$ æœ‰ $n$ ä¸ªkeys. æˆ‘ä»¬å¯ä»¥é€šè¿‡çŸ©é˜µè¿ç®—çš„æ–¹å¼è®¡ç®—æ‰€æœ‰ $mn$ ä¸ªscoreï¼š
    
    <p align="center">
    $$
    ğ›¼(ğª,ğ¤)=âŸ¨ğª,ğ¤âŸ©/ \sqrt{d}
    $$
    </p>

    ```
    # Save to the d2l package.
    class DotProductAttention(nn.Module): 
        def __init__(self, dropout, **kwargs):
            super(DotProductAttention, self).__init__(**kwargs)
            self.dropout = nn.Dropout(dropout)
    
        # query: (batch_size, #queries, d)
        # key: (batch_size, #kv_pairs, d)
        # value: (batch_size, #kv_pairs, dim_v)
        # valid_length: either (batch_size, ) or (batch_size, xx)
        def forward(self, query, key, value, valid_length=None):
            d = query.shape[-1]
            # set transpose_b=True to swap the last two dimensions of key
            
            scores = torch.bmm(query, key.transpose(1,2)) / math.sqrt(d)
            attention_weights = self.dropout(masked_softmax(scores, valid_length))
            print("attention_weight\n",attention_weights)
            return torch.bmm(attention_weights, value)
    ```
  
    
  
  - **å¤šå±‚æ„ŸçŸ¥æœºæ³¨æ„åŠ› Multilayer Perceptron Attention**
  
    - åœ¨å¤šå±‚æ„ŸçŸ¥å™¨ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå°† query and keys æŠ•å½±åˆ°  $â„^â„$ .ä¸ºäº†æ›´å…·ä½“ï¼Œæˆ‘ä»¬å°†å¯ä»¥å­¦ä¹ çš„å‚æ•°åšå¦‚ä¸‹æ˜ å°„ $ğ–_ğ‘˜âˆˆâ„^{â„Ã—ğ‘‘_ğ‘˜}$ ,  $ğ–_ğ‘âˆˆâ„^{â„Ã—ğ‘‘_ğ‘}$ , and  $ğ¯âˆˆâ„^h$ . å°†scoreå‡½æ•°å®šä¹‰
      
      <p align="center">
      $$
      ğ›¼(ğ¤,ğª)=ğ¯^ğ‘‡tanh(ğ–_ğ‘˜ğ¤+ğ–_ğ‘ğª)
      $$
      </p>
      
      ç„¶åå°†key å’Œ value åœ¨ç‰¹å¾çš„ç»´åº¦ä¸Šåˆå¹¶ï¼ˆconcatenateï¼‰ï¼Œç„¶åé€è‡³ a single hidden layer perceptron è¿™å±‚ä¸­ hidden layer ä¸º  â„  and è¾“å‡ºçš„sizeä¸º 1 .éšå±‚æ¿€æ´»å‡½æ•°ä¸ºtanhï¼Œæ— åç½®.
    
    ```
    # Save to the d2l package.
    class MLPAttention(nn.Module):  
        def __init__(self, units,ipt_dim,dropout, **kwargs):
            super(MLPAttention, self).__init__(**kwargs)
            # Use flatten=True to keep query's and key's 3-D shapes.
            self.W_k = nn.Linear(ipt_dim, units, bias=False)
            self.W_q = nn.Linear(ipt_dim, units, bias=False)
            self.v = nn.Linear(units, 1, bias=False)
            self.dropout = nn.Dropout(dropout)
    
        def forward(self, query, key, value, valid_length):
            query, key = self.W_k(query), self.W_q(key)
            #print("size",query.size(),key.size())
            # expand query to (batch_size, #querys, 1, units), and key to
            # (batch_size, 1, #kv_pairs, units). Then plus them with broadcast.
            features = query.unsqueeze(2) + key.unsqueeze(1)
            #print("features:",features.size())  #--------------å¼€å¯
            scores = self.v(features).squeeze(-1) 
            attention_weights = self.dropout(masked_softmax(scores, valid_length))
            return torch.bmm(attention_weights, value)
    ```
    
  
- **Sequence to Sequence with Attention Mechanisms**

  - Here, the memory of the attention layer consists of **all the information** that the encoder has seenâ€”**the encoder output at each timestep**. During the decoding, the **decoder output** from the **previous timestep** $tâˆ’1$ is used as the **query**. The **output of the attention model** is viewed as the **context information**, and such **context** is **concatenated** with the **decoder input** $D_t$. Finally, we feed the concatenation into the decoder.

    <p align="center">
      <img src="https://d2l.ai/_images/seq2seq_attention.svg" style="zoom:100%" />
    </p>

    <p align="center">
      <img src="https://d2l.ai/_images/seq2seq-attention-details.svg" style="zoom:100%" />
    </p>

    - we initialize the state of the decoder by passing three items from the encoder:
      - **the encoder outputs of all timesteps**: they are used as the attention layerâ€™s memory with **identical keys and values**;**ï¼ˆkeysä¸valuesç›¸åŒï¼‰**
      - **the hidden state of the encoderâ€™s final timestep**: it is used as the **initial decoderâ€™s hidden state**;
      - **the encoder valid length**: so the attention layer will not consider the padding tokens with in the encoder outputs.

    ```
    class Seq2SeqAttentionDecoder(d2l.Decoder):
        def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                     dropout=0, **kwargs):
            super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)
            self.attention_cell = MLPAttention(num_hiddens,num_hiddens, dropout)
            self.embedding = nn.Embedding(vocab_size, embed_size)
            self.rnn = nn.LSTM(embed_size+ num_hiddens,num_hiddens, num_layers, dropout=dropout)
            self.dense = nn.Linear(num_hiddens,vocab_size)
    
        def init_state(self, enc_outputs, enc_valid_len, *args):
            outputs, hidden_state = enc_outputs
    #         print("first:",outputs.size(),hidden_state[0].size(),hidden_state[1].size())
            # Transpose outputs to (batch_size, seq_len, hidden_size)
            return (outputs.permute(1,0,-1), hidden_state, enc_valid_len)
            #outputs.swapaxes(0, 1)
            
        def forward(self, X, state):
            enc_outputs, hidden_state, enc_valid_len = state
            #("X.size",X.size())
            X = self.embedding(X).transpose(0,1)
    #         print("Xembeding.size2",X.size())
            outputs = []
            for l, x in enumerate(X):
    #             print(f"\n{l}-th token")
    #             print("x.first.size()",x.size())
                # query shape: (batch_size, 1, hidden_size)
                # select hidden state of the last rnn layer as query
                query = hidden_state[0][-1].unsqueeze(1) # np.expand_dims(hidden_state[0][-1], axis=1)
                # context has same shape as query
    #             print("query enc_outputs, enc_outputs:\n",query.size(), enc_outputs.size(), enc_outputs.size())
                context = self.attention_cell(query, enc_outputs, enc_outputs, enc_valid_len)
                # Concatenate on the feature dimension
    #             print("context.size:",context.size())
                x = torch.cat((context, x.unsqueeze(1)), dim=-1)
                # Reshape x to (1, batch_size, embed_size+hidden_size)
    #             print("rnn",x.size(), len(hidden_state))
                out, hidden_state = self.rnn(x.transpose(0,1), hidden_state)
                outputs.append(out)
            outputs = self.dense(torch.cat(outputs, dim=0))
            return outputs.transpose(0, 1), [enc_outputs, hidden_state,
                                            enc_valid_len]
    ```

    

### Transformer

- **self-attention**

  - The self-attention model is a normal attention model, with **its query, its key, and its value being copied exactly the same from each item of the sequential inputs**. self-attention **outputs** a **same-length sequential output for each input item**. Compared with a recurrent layer, output items of a self-attention layer can be **computed in parallel and, therefore, it is easy to obtain a highly-efficient implementation**.

    <p align="center">
      <img src="https://d2l.ai/_images/self-attention.svg" style="zoom:100%" />
    </p>

- **multi-head attention**

  - **åœ¨Transformeræ¨¡å‹ä¸­ï¼Œæ³¨æ„åŠ›å¤´æ•°ä¸ºhï¼ŒåµŒå…¥å‘é‡å’Œéšè—çŠ¶æ€ç»´åº¦å‡ä¸ºdï¼Œé‚£ä¹ˆä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚æ‰€å«çš„å‚æ•°é‡æ˜¯ï¼š**

    - *h*ä¸ªæ³¨æ„åŠ›å¤´ä¸­ï¼Œæ¯ä¸ªçš„å‚æ•°é‡ä¸º$3d^2$ï¼Œæœ€åçš„è¾“å‡ºå±‚å½¢çŠ¶ä¸º$hd \times d$ï¼Œæ‰€ä»¥å‚æ•°é‡å…±ä¸º$4hd^2$ã€‚
  
  - The *multi-head attention* layer **consists of $h$ parallel self-attention layers**, **each one is called a head**. For each head, before feeding into the attention layer, we project the queries, keys, and values with three dense layers with hidden sizes $p_q$, $p_k$, and $p_v$, respectively. The outputs of these $h$ attention heads are **concatenated** and then processed by a final dense layer.

    <p align="center">
    <img src="https://d2l.ai/_images/multi-head-attention.svg" style="zoom:100%" />
    </p>

  - Assume that the dimension for a query, a key, and a value are $d_q$, $d_k$, and $d_v$, respectively. Then, for each head $ğ‘–=1,â€¦,â„$, we can train learnable parameters $ğ–^{(ğ‘–)}_ğ‘âˆˆâ„^{ğ‘_ğ‘Ã—ğ‘‘_ğ‘}$, $ğ–^{(ğ‘–)}_ğ‘˜âˆˆâ„^{ğ‘_ğ‘˜Ã—ğ‘‘_ğ‘˜}$, and $ğ–^{(ğ‘–)}_ğ‘£âˆˆâ„^{ğ‘_ğ‘£Ã—ğ‘‘_ğ‘£}$. Therefore, the output for each head is $ğ¨^{(ğ‘–)}=attention(ğ–^{(ğ‘–)}_ğª,ğ–^{(ğ‘–)}_ğ¤,ğ–^{(ğ‘–)}_ğ‘£ğ¯)$,where attention can be any attention layer, such as the `DotProductAttention` and `MLPAttention`. 

    After that, the output with length $p_v$ from each of the $h$ attention heads are concatenated to be an output of length $hp_v$, which is then passed the final dense layer with $d_o$ hidden units. The weights of this dense layer can be denoted by $ğ–ğ‘œâˆˆâ„^{ğ‘‘_ğ‘œÃ—â„ğ‘_ğ‘£}$. As a result, the multi-head attention output will be $\mathbf{o}=\mathbf{W}_{o}\left[\begin{array}{c}{\mathbf{o}^{(1)}} \\ {\vdots} \\ {\mathbf{o}^{(h)}}\end{array}\right]$.
  
    ```
    class MultiHeadAttention(nn.Module):
    		"""
    		Assume that the multi-head attention contain the number heads num_heads  =â„ , the hidden size hidden_size  =ğ‘ğ‘=ğ‘ğ‘˜=ğ‘ğ‘£  are the same for the query, key, and value dense layers. In addition, since the multi-head attention keeps the same dimensionality between its input and its output, we have the output feature size  ğ‘‘ğ‘œ=  hidden_size as well.
    		"""
        def __init__(self, input_size, hidden_size, num_heads, dropout, **kwargs):
            super(MultiHeadAttention, self).__init__(**kwargs)
            self.num_heads = num_heads
            self.attention = DotProductAttention(dropout)
            self.W_q = nn.Linear(input_size, hidden_size, bias=False)
            self.W_k = nn.Linear(input_size, hidden_size, bias=False)
            self.W_v = nn.Linear(input_size, hidden_size, bias=False)
            self.W_o = nn.Linear(hidden_size, hidden_size, bias=False)
        
        def forward(self, query, key, value, valid_length):
            # query, key, and value shape: (batch_size, seq_len, dim),
            # where seq_len is the length of input sequence
            # valid_length shape is either (batch_size, )
            # or (batch_size, seq_len).
    
            # Project and transpose query, key, and value from
            # (batch_size, seq_len, hidden_size * num_heads) to
            # (batch_size * num_heads, seq_len, hidden_size).
            
            query = transpose_qkv(self.W_q(query), self.num_heads)
            key = transpose_qkv(self.W_k(key), self.num_heads)
            value = transpose_qkv(self.W_v(value), self.num_heads)
            
            if valid_length is not None:
                # Copy valid_length by num_heads times
                device = valid_length.device
                valid_length = valid_length.cpu().numpy() if valid_length.is_cuda else valid_length.numpy()
                if valid_length.ndim == 1:
                    valid_length = torch.FloatTensor(np.tile(valid_length, self.num_heads))
                else:
                    valid_length = torch.FloatTensor(np.tile(valid_length, (self.num_heads,1)))
    
                valid_length = valid_length.to(device)
            
            output = self.attention(query, key, value, valid_length)
            
            # Transpose from (batch_size * num_heads, seq_len, hidden_size) back
            # to (batch_size, seq_len, hidden_size * num_heads)
            output_concat = transpose_output(output, self.num_heads)
            return self.W_o(output_concat)
    ```
  
    ```
    def transpose_qkv(X, num_heads):
        # Original X shape: (batch_size, seq_len, hidden_size * num_heads),
        # -1 means inferring its value, after first reshape, X shape:
        # (batch_size, seq_len, num_heads, hidden_size)
        X = X.view(X.shape[0], X.shape[1], num_heads, -1)
        
        # After transpose, X shape: (batch_size, num_heads, seq_len, hidden_size)
        X = X.transpose(2, 1).contiguous()
    
        # Merge the first two dimensions. Use reverse=True to infer shape from
        # right to left.
        # output shape: (batch_size * num_heads, seq_len, hidden_size)
        output = X.view(-1, X.shape[2], X.shape[3])
        return output
    
    
    # Saved in the d2l package for later use
    def transpose_output(X, num_heads):
        # A reversed version of transpose_qkv
        X = X.view(-1, num_heads, X.shape[1], X.shape[2])
        X = X.transpose(2, 1).contiguous()
        return X.view(X.shape[0], X.shape[1], -1)
    ```
  
- **Position-wise Feed-Forward Networks**

  - Transformer æ¨¡å—å¦ä¸€ä¸ªéå¸¸é‡è¦çš„éƒ¨åˆ†å°±æ˜¯åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ï¼Œå®ƒæ¥å—ä¸€ä¸ªå½¢çŠ¶ä¸ºï¼ˆbatch_sizeï¼Œseq_length, feature_sizeï¼‰çš„ä¸‰ç»´å¼ é‡ã€‚Position-wise FFNç”±ä¸¤ä¸ªå…¨è¿æ¥å±‚ç»„æˆï¼Œä»–ä»¬ä½œç”¨åœ¨æœ€åä¸€ç»´ä¸Šã€‚å› ä¸ºåºåˆ—çš„æ¯ä¸ªä½ç½®çš„çŠ¶æ€éƒ½ä¼šè¢«å•ç‹¬åœ°æ›´æ–°ï¼Œæ‰€ä»¥æˆ‘ä»¬ç§°ä»–ä¸ºposition-wiseï¼Œè¿™ç­‰æ•ˆäºä¸€ä¸ª1x1çš„å·ç§¯ã€‚

  - ä¸å¤šå¤´æ³¨æ„åŠ›å±‚ç›¸ä¼¼ï¼Œ**FFNå±‚åŒæ ·åªä¼šå¯¹æœ€åä¸€ç»´çš„å¤§å°è¿›è¡Œæ”¹å˜**

    ```python
    class PositionWiseFFN(nn.Module):
      	"""
      	>>> ffn = PositionWiseFFN(4, 4, 8)
    				out = ffn(torch.ones((2,3,4)))
    
    		>>> print(out, out.shape)
    				torch.Size([2, 3, 8])
      	"""
        def __init__(self, input_size, ffn_hidden_size, hidden_size_out, **kwargs):
            super(PositionWiseFFN, self).__init__(**kwargs)
            self.ffn_1 = nn.Linear(input_size, ffn_hidden_size)
            self.ffn_2 = nn.Linear(ffn_hidden_size, hidden_size_out)
            
            
        def forward(self, X):
            return self.ffn_2(F.relu(self.ffn_1(X)))
    ```

- **ç›¸åŠ å½’ä¸€åŒ–å±‚ Add and Norm**

  - ç›¸åŠ å½’ä¸€åŒ–å±‚å¯ä»¥**å¹³æ»‘åœ°æ•´åˆè¾“å…¥å’Œå…¶ä»–å±‚çš„è¾“å‡º**ï¼Œå› æ­¤æˆ‘ä»¬**åœ¨æ¯ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚å’ŒFFNå±‚åé¢éƒ½æ·»åŠ ä¸€ä¸ªå«æ®‹å·®è¿æ¥çš„Layer Normå±‚**ã€‚è¿™é‡Œ Layer Norm ä¸Batch Normå¾ˆç›¸ä¼¼ï¼Œå”¯ä¸€çš„åŒºåˆ«åœ¨äºBatch Normæ˜¯å¯¹äºbatch sizeè¿™ä¸ªç»´åº¦è¿›è¡Œè®¡ç®—å‡å€¼å’Œæ–¹å·®çš„ï¼Œè€ŒLayer Normåˆ™æ˜¯å¯¹æœ€åä¸€ç»´è¿›è¡Œè®¡ç®—ã€‚**å±‚å½’ä¸€åŒ–å¯ä»¥é˜²æ­¢å±‚å†…çš„æ•°å€¼å˜åŒ–è¿‡å¤§ï¼Œä»è€Œæœ‰åˆ©äºåŠ å¿«è®­ç»ƒé€Ÿåº¦å¹¶ä¸”æé«˜æ³›åŒ–æ€§èƒ½ã€‚**

    ```python
    class AddNorm(nn.Module):
      	"""
      	>>> add_norm = AddNorm(4, 0.5)
    				add_norm(torch.ones((2,3,4)), torch.ones((2,3,4))).shape
    		>>> torch.Size([2, 3, 4])
      	"""
        def __init__(self, hidden_size, dropout, **kwargs):
            super(AddNorm, self).__init__(**kwargs)
            self.dropout = nn.Dropout(dropout)
            self.norm = nn.LayerNorm(hidden_size)
        
        def forward(self, X, Y):
            return self.norm(self.dropout(Y) + X)
    ```

- **ä½ç½®ç¼–ç **

  - ä¸å¾ªç¯ç¥ç»ç½‘ç»œä¸åŒï¼Œæ— è®ºæ˜¯å¤šå¤´æ³¨æ„åŠ›ç½‘ç»œè¿˜æ˜¯å‰é¦ˆç¥ç»ç½‘ç»œéƒ½æ˜¯ç‹¬ç«‹åœ°å¯¹æ¯ä¸ªä½ç½®çš„å…ƒç´ è¿›è¡Œæ›´æ–°ï¼Œè¿™ç§ç‰¹æ€§å¸®åŠ©æˆ‘ä»¬å®ç°äº†é«˜æ•ˆçš„å¹¶è¡Œï¼Œå´ä¸¢å¤±äº†é‡è¦çš„åºåˆ—é¡ºåºçš„ä¿¡æ¯ã€‚ä¸ºäº†æ›´å¥½çš„æ•æ‰åºåˆ—ä¿¡æ¯ï¼ŒTransformeræ¨¡å‹å¼•å…¥äº†ä½ç½®ç¼–ç å»ä¿æŒè¾“å…¥åºåˆ—å…ƒç´ çš„ä½ç½®ã€‚

    <p align="center">
      <img src="https://d2l.ai/_images/positional_encoding.svg" style="zoom:100%" />
    </p>

- **ç¼–ç å™¨**

  <p align="center">
    <img src="https://d2l.ai/_images/transformer.svg" style="zoom:100%" />
  </p>

  - æˆ‘ä»¬å·²ç»æœ‰äº†ç»„æˆTransformerçš„å„ä¸ªæ¨¡å—ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥å¼€å§‹æ­å»ºäº†ï¼**ç¼–ç å™¨åŒ…å«ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚ï¼Œä¸€ä¸ªposition-wise FFNï¼Œå’Œä¸¤ä¸ª Add and Normå±‚**ã€‚å¯¹äº**<font color=red>attentionæ¨¡å‹ä»¥åŠFFNæ¨¡å‹ï¼Œæˆ‘ä»¬çš„è¾“å‡ºç»´åº¦éƒ½æ˜¯ä¸embeddingç»´åº¦ä¸€è‡´çš„</font>**ï¼Œè¿™ä¹Ÿæ˜¯ç”±äºæ®‹å·®è¿æ¥å¤©ç”Ÿçš„ç‰¹æ€§å¯¼è‡´çš„ï¼Œå› ä¸ºæˆ‘ä»¬è¦å°†å‰ä¸€å±‚çš„è¾“å‡ºä¸åŸå§‹è¾“å…¥ç›¸åŠ å¹¶å½’ä¸€åŒ–ã€‚

    ```python
    class EncoderBlock(nn.Module):
      	"""
      	# batch_size = 2, seq_len = 100, embedding_size = 24
    		# ffn_hidden_size = 48, num_head = 8, dropout = 0.5
    
        >>>	X = torch.ones((2, 100, 24))
            encoder_blk = EncoderBlock(24, 48, 8, 0.5)
            encoder_blk(X, valid_length).shape
        >>> torch.Size([2, 100, 24])
      	"""
        def __init__(self, embedding_size, ffn_hidden_size, num_heads,
                     dropout, **kwargs):
            super(EncoderBlock, self).__init__(**kwargs)
            self.attention = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)
            self.addnorm_1 = AddNorm(embedding_size, dropout)
            self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)
            self.addnorm_2 = AddNorm(embedding_size, dropout)
    
        def forward(self, X, valid_length):
            Y = self.addnorm_1(X, self.attention(X, X, X, valid_length))
            return self.addnorm_2(Y, self.ffn(Y))
    ```

- **æ•´ä¸ªTransformer ç¼–ç å™¨æ¨¡å‹**

  - æ•´ä¸ªç¼–ç å™¨ç”±nä¸ªåˆšåˆšå®šä¹‰çš„Encoder Blockå †å è€Œæˆ

    ```python
    class TransformerEncoder(d2l.Encoder):
      	"""
      	# test encoder
        >>> encoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5)
        		encoder(torch.ones((2, 100)).long(), valid_length).shape
        >>> torch.Size([2, 100, 24])
      	"""
      	
        def __init__(self, vocab_size, embedding_size, ffn_hidden_size,
                     num_heads, num_layers, dropout, **kwargs):
            super(TransformerEncoder, self).__init__(**kwargs)
            self.embedding_size = embedding_size
            self.embed = nn.Embedding(vocab_size, embedding_size)
            self.pos_encoding = PositionalEncoding(embedding_size, dropout)
            self.blks = nn.ModuleList()
            for i in range(num_layers):
                self.blks.append(
                    EncoderBlock(embedding_size, ffn_hidden_size,
                                 num_heads, dropout))
    
        def forward(self, X, valid_length, *args):
            X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))
            for blk in self.blks:
                X = blk(X, valid_length)
            return X
    ```
  
- **è§£ç å™¨**
  
  <p align="center">
    <img src="https://d2l.ai/_images/self-attention-predict.svg" style="zoom:100%" />
  </p>
  - The Transformer decoder block looks similar to the Transformer encoder block. However, besides the two sub-layersâ€”the multi-head attention layer and the positional encoding network, the decoder Transformer block contains a **third sub-layer**, which **applies multi-head attention on the output of the encoder stack**. To be specific, **at timestep $t$**, assume that **$x_t$ is the current input**, i.e., the query. the keys and values of the self-attention layer consist of the current query with all the **past queries** $ğ±_1,â€¦,ğ±_{ğ‘¡âˆ’1}.$
  
  - During training, the **output** for the $t$-â€‹queryâ€‹ could **observe all the previous key-value pairs**. It results in an different behavior from prediction. Thus, during prediction we can eliminate the unnecessary information by specifying the valid length to be $ğ‘¡$ for the $ğ‘¡^{th}$ query.
  
    ```
    class DecoderBlock(nn.Module):
        def __init__(self, embedding_size, ffn_hidden_size, num_heads,dropout,i,**kwargs):
            super(DecoderBlock, self).__init__(**kwargs)
            self.i = i
            self.attention_1 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)
            self.addnorm_1 = AddNorm(embedding_size, dropout)
            self.attention_2 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)
            self.addnorm_2 = AddNorm(embedding_size, dropout)
            self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)
            self.addnorm_3 = AddNorm(embedding_size, dropout)
        
        def forward(self, X, state):
            enc_outputs, enc_valid_length = state[0], state[1]
            
            # state[2][self.i] stores all the previous t-1 query state of layer-i
            # len(state[2]) = num_layers
            
            if state[2][self.i] is None:
                key_values = X
            else:
                # shape of key_values = (batch_size, t, hidden_size)
                key_values = torch.cat((state[2][self.i], X), dim=1) 
            state[2][self.i] = key_values
            
            if self.training:
                batch_size, seq_len, _ = X.shape
                # Shape: (batch_size, seq_len), the values in the j-th column are j+1
                valid_length = torch.FloatTensor(np.tile(np.arange(1, seq_len+1), (batch_size, 1))) 
                valid_length = valid_length.to(X.device)
            else:
                valid_length = None
    
            X2 = self.attention_1(X, key_values, key_values, valid_length)
            Y = self.addnorm_1(X, X2)
            Y2 = self.attention_2(Y, enc_outputs, enc_outputs, enc_valid_length)
            Z = self.addnorm_2(Y, Y2)
            return self.addnorm_3(Z, self.ffn(Z)), state
    ```
  
    ```
    class TransformerDecoder(d2l.Decoder):
        def __init__(self, vocab_size, embedding_size, ffn_hidden_size,
                     num_heads, num_layers, dropout, **kwargs):
            super(TransformerDecoder, self).__init__(**kwargs)
            self.embedding_size = embedding_size
            self.num_layers = num_layers
            self.embed = nn.Embedding(vocab_size, embedding_size)
            self.pos_encoding = PositionalEncoding(embedding_size, dropout)
            self.blks = nn.ModuleList()
            for i in range(num_layers):
                self.blks.append(
                    DecoderBlock(embedding_size, ffn_hidden_size, num_heads,
                                 dropout, i))
            self.dense = nn.Linear(embedding_size, vocab_size)
    
        def init_state(self, enc_outputs, enc_valid_length, *args):
            return [enc_outputs, enc_valid_length, [None]*self.num_layers]
    
        def forward(self, X, state):
            X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))
            for blk in self.blks:
                X, state = blk(X, state)
            return self.dense(X), state
    ```
  
    
  
