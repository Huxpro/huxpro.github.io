---
title: "brief summary of papers"
subtitle: "nmt"
layout: post
author: "echisenyang"
header-style: text
hidden: true
catalog: true
tags:
  - nmt
---





## ã€‡ã€æ€»ç»“

- **traditional phrase-based v.s NMT**

  Back in the old days, traditional phrase-based translation systems performed their task by breaking up source sentences into multiple chunks and then translated them phrase-by-phrase. This led to disfluency in the translation outputs and was not quite like how we, humans, translate. We read the entire source sentence, understand its meaning, and then produce a translation. Neural Machine Translation (NMT) mimics that!

  - By using encoder-decoder architecture, **NMT addresses the local translation problem in the traditional phrase-based approach**: it can capture *long-range dependencies* in languages, e.g., gender agreements; syntax structures; etc., and produce much more fluent translations
  - 

## ä¸€ã€Paraphrase

### 1.Neural Clinical Paraphrase Generation with Attention [2015,huawei]

Clinical paraphrase generation is especially vital in building patient-centric clinical decision support (CDS) applications where users are able to understand complex clinical jargons via easily comprehensible alternative paraphrases.

> ä¸“ä¸šé¢†åŸŸèƒŒæ™¯ï¼šä¸´åºŠåŒ»å­¦

Unlike **bilingual** machine translation, **monolingual** machine translation considers the source language the same as the target language, which allows for its adaptation as a paraphrase generation task.

> å­¦æœ¯èƒŒæ™¯ï¼šmonolingual machine translationï¼ˆå•ä¸€è¯­ç§ï¼‰ã€paraphrase generation task

We propose an **end-to-end neural network** built on an **attention-based bidirectional Recurrent Neural Network (RNN) architecture** with an **encoderdecoder framework** to perform the task. Conventional bilingual NMT models mostly rely on word-level modeling and are often **limited by out-of-vocabulary (OOV)** issues. In contrast, we represent the **source and target paraphrase pairs** as character sequences to address this limitation. 

> end-to-endã€attention-basedã€biRNNã€encoderdecoderã€æ”¯æŒword-level/character-level
>
> â“ source and target paraphrase pairs

**è¯„ä¼°ç»“æœï¼šæœªæä¾›å®Œæ•´çš„å®ç°ï¼Œåªæä¾›äº†RNNæ¨¡å—çš„å®ç°https://github.com/lisa-groundhog/GroundHogï¼ˆRNN templates provided by the GroundHog libraryï¼‰ï¼Œç¯å¢ƒåŸºäºTheanoï¼Œå®éªŒç»“æœåå‘äºçŸ­è¯­çš„ç»„åˆè€Œä¸æ˜¯å®Œæ•´çš„å¥å­ã€‚**

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/1Uelw8.png" alt="1Uelw8" style="zoom:33%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/HqluPS.png" alt="HqluPS" style="zoom:50%;" />

### 

### 2.Decomposable Neural Paraphrase Generation [2019,huawei]

**Paraphrases** are texts that convey the same meaning using different wording. (**Paraphraseså®šä¹‰**)

**Paraphrase generation** is an important technique in natural language processing (NLP)

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/a0sMop.png" alt="a0sMop" style="zoom:50%;" />

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/2sTced.png" alt="2sTced" style="zoom:50%;" />

- **Separator**

  In this work we employ the stacked LSTMs to compute the distribution of the latent variables recursively

- **Multi-granularity encoder and decoder**

  Based on the Transformer design in Vaswani et al. (2017), each encoder or decoder is composed of positional encoding, stacked multihead attention, layer normalization, and feedforward neural networks.

  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/xBuooR.png" alt="xBuooR" style="zoom:50%;" />

- **Aggregator**

  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/UF1MGR.png" alt="UF1MGR" style="zoom:50%;" />

è¯„ä¼°ï¼šæ˜¯ä¸€ä¸ªæ¯”è¾ƒæ–°çš„æ€è·¯ï¼Œä½†æ˜¯æ²¡æœ‰ç›¸åº”çš„å®ç°ï¼Œè€Œä¸”å¯¹æ•°æ®çš„è¦æ±‚æ¯”è¾ƒé«˜ï¼ˆ**ç›¸åŒæ¨¡å¼**ï¼‰

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/hXPIOs.png" alt="hXPIOs" style="zoom:67%;" />

### 3.Query and Output: Generating Words by Querying Distributed Word Representations for Paraphrase Generationã€2018ã€‘

---

**Abstract**

- èƒŒæ™¯ï¼šç¿»è¯‘çš„å¥å­è¯­æ³•æ­£ç¡®ä½†æ˜¯è¯­ä¹‰æœ‰é—®é¢˜

**Most recent approaches use the sequenceto-sequence model for paraphrase generation**. The existing sequence-to-sequence model tends to memorize the words and the patterns in the training dataset instead of learning the meaning of the words. **Therefore, the generated sentences are often grammatically correct but semantically improper.**

- how to evaluate

Following previous work, we evaluate our model on two paraphrase-oriented tasks, namely **text simpliï¬cation** and **short text abstractive summarization**.

---

**Introduction**

- definition

  **Paraphrase is a restatement of the meaning of a text using other words**. Many natural language generation tasks are paraphrase-orientated, such as text simpliï¬cation and short text summarization.

- problem

  One of the problem is that the **existing sequence-to-sequence model tends to memorize the words and the patterns in the training dataset instead of the meaning of the words**.

---

**Proposed Model**

![4kjomi](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/4kjomi.png)

- Word Embedding Attention Network is based on the encoder-decoder framework, which consists of two components: **a source text encoder, and a target text decoder**.
  - Given the source texts, the **encoder** compresses the **source texts into dense representation vectors**, and the **decoder** generates the **paraphrased texts**.
  - Our proposed model **generates the words by querying distributed word representations (i.e. neural word embeddings), hoping to capturing the meaning of the according words.**

---

**æœ€å¤§åˆ›æ–°ç‚¹ï¼šåŠ å…¥äº†æ›´æ–°word embeddingçš„queryæœºåˆ¶ï¼Œthe word embedding is updated from three sources: the input of the encoder, the input of the decoder, and the query of the output layer. è¾¾åˆ°æ•æ‰è¯­ä¹‰ä¿¡æ¯çš„ç›®çš„ï¼Œä½†æ˜¯æœ¬è´¨è¿˜æ˜¯encoderdecoder framework**

åŸºäºpytorchå®ç° https://github.com/lancopku/WEAN

### 4.Paraphrase Generation with Latent Bag of Words

**Introduction**

- Paraphrases are **deï¬ned** as sentences conveying the same meaning but with different surface realization.

![QkOGIs](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/QkOGIs.png)

- we propose a **hierarchical latent bag of words mode**l for planning and realization. Our model uses words of the **source sentence** to **predict** their **neighbors** in the bag of words from target sentences

![8oTAPA](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/8oTAPA.png)

**æ€»ç»“ï¼šseq2seqæ¨¡å‹ï¼ŒåŠ å…¥äº†cbowåŠ å¼ºè¯­ä¹‰ä¿¡æ¯**

tensorflowå®ç° https://github.com/FranxYao/dgm_latent_bow

### 5.Automatic Compilation of Resources for Academic Writing and Evaluating with Informal Word Identiï¬cation and Paraphrasing System

![Wzs7u5](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Wzs7u5.png)

æ€»ç»“ï¼šåšçš„æ˜¯Academic writingï¼Œå°±æ˜¯æŠŠæŸäº›è¯æ›¿æ¢ä¸ºæ›´å­¦æœ¯ä¸“ä¸šçš„è¯ï¼Œä¸ç¬¦åˆ

### 6.Learning Semantic Sentence Embeddings using Pair-wise Discriminator

![gr8SDU](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/gr8SDU.png)

æ€»ç»“ï¼šencoder-decorderæ¡†æ¶+pair-wise discriminator

pytorchç‰ˆ https://github.com/badripatro/PQG

### 7.Building a Non-Trivial Paraphrase Corpus using Multiple Machine Translation Systems

æ€»ç»“ Paraphrase Identificationé¢†åŸŸæ–‡ç« ï¼ŒParaphrase Identificationçš„ä»»åŠ¡æ˜¯Given any pair of sentences, automatically identifies whether these two sentences are paraphrasesã€‚



### äºŒã€NMT

### 1.NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE [2015,ICLR,Yoshua Bengio]

The models proposed recently for neural machine translation often belong to **a family of encoderâ€“decoders** and **encode a source sentence into a ï¬xed-length vector** from which a decoder generates a translation. 

> In this paper, we **conjecture that the use of a ï¬xed-length vector is a bottleneck** in improving the performance of this basic encoderâ€“decoder architecture, and propose to extend this by allowing a model to **automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly.(æ‰“ç ´äº†encodeè¾“å‡ºæ˜¯fix-lengthçš„å±€é™)**

> **In order to address this issue, we introduce an extension to** the encoderâ€“decoder model which learns to align and translate jointly. Each time the proposed model generates a word in a translation, it (soft-)searches for **a set of positions in a source sentence (a set of input words)** where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.

**æœªè§£å†³çš„æŒ‘æˆ˜**ï¼šOne of challenges left for the future is to better handle unknown, or rare words. This will be required for the model to be more widely used and to match the performance of current state-of-the-art machine translation systems in all contexts.

<img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/drbcGv.png" alt="drbcGv" style="zoom:50%;" />

æ¡†æ¶ä¸Neural Clinical Paraphrase Generation with Attention [2015,huawei]ä¸€è‡´ï¼Œå®ç°çš„å¤ç°ä»£ç éƒ½æ˜¯åŸºäºhttps://github.com/lisa-groundhog/GroundHogï¼ŒåŒæ ·ä¹Ÿæ˜¯åŸºäºTheano

è¿˜æœ‰ä¸€ä¸ª5kğŸŒŸçš„tensorflowç‰ˆæœ¬ï¼Œè¿™ä¸ªè¿˜æŒºé è°± https://github.com/tensorflow/nmt

### 2.Googleâ€™s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation [2016,google]

Unfortunately, NMT systems are known to be **computationally expensive** both in training and in translation inference â€“ sometimes prohibitively so in the case of very large data sets and large models. Several authors have also charged that NMT systems **lack robustness**, particularly when input sentences contain rare words. These issues have **hindered NMTâ€™s use in practical deployments and services**, where both accuracy and speed are essential.

> GNMTå°±æ˜¯å¤„ç†è¿™äº›issuesçš„ï¼Œè®­ç»ƒé¢„æµ‹æé€Ÿï¼Œè§£å†³robustnessé—®é¢˜

**Three inherent weaknesses of Neural Machine Translation** are responsible for this gap: 

- its slower training and inference speed, 

  > **residual** connections

- ineï¬€ectiveness in dealing with rare words, 

  > There are two broad categories of approaches to address the translation of out-of-vocabulary (OOV) words. **One approach is to simply copy rare words from source to target**. Another broad category of approaches is to **use sub-word units**, e.g., chararacters [10], mixed word/characters [28], or more intelligent sub-words [38]. (**wordpiece**)

- and sometimes failure to translate all words in the source sentence.

  > decoderé˜¶æ®µç”¨beam search technique

![TnmVRt](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/TnmVRt.png)

å®ç°ï¼šæ‰¾åˆ°äº†ä¸€ä¸ª**éå®˜æ–¹ä¸å†ç»´æŠ¤çš„tensorflowç‰ˆæœ¬**https://github.com/shawnxu1318/Google-Neural-Machine-Translation-GNMT

![z9PjoK](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/z9PjoK.png)

â“ ä¸­æ–‡æœ‰wordpieceå—ï¼Ÿ

### 3.OpenNMT: Open-Source Toolkit for Neural Machine Translation [2017,harvard]

4k ğŸŒŸPyTorch https://github.com/OpenNMT/OpenNMT-py

1k ğŸŒŸtfç‰ˆæœ¬  https://github.com/OpenNMT/OpenNMT-tf
ä½¿ç”¨æ–‡æ¡£ https://opennmt.net/OpenNMT/

![4ge1dp](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/4ge1dp.jpg)

![qZQgrI](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/qZQgrI.png)

![QCHsyh](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/QCHsyh.png)

![Bg5stA](https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Bg5stA.png)





