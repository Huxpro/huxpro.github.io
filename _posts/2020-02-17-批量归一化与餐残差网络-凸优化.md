---
layout:     post
title:      PyTorchç³»åˆ—ä¹‹æ‰¹é‡å½’ä¸€åŒ–å’Œæ®‹å·®ç½‘ç»œï½œå‡¸ä¼˜åŒ–ï½œæ¢¯åº¦ä¸‹é™
subtitle:   æ‰¹é‡å½’ä¸€åŒ–å’Œæ®‹å·®ç½‘ç»œï½œå‡¸ä¼˜åŒ–ï½œæ¢¯åº¦ä¸‹é™
date:       2020-02-16
author:     Young
header-img: img/bg-post/1*_SynSRVD2QrdmEXXJ2wVMA.png
catalog: true
tags:
    - PyTorch
---

## Task06ï¼šæ‰¹é‡å½’ä¸€åŒ–å’Œæ®‹å·®ç½‘ç»œï½œå‡¸ä¼˜åŒ–ï½œæ¢¯åº¦ä¸‹é™

### æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰

Training deep neural nets is difficult. And getting them to **converge in a reasonable amount of time can be tricky**. In this section, we describe **batch normalization (BN)** [[Ioffe & Szegedy, 2015\]](https://d2l.ai/chapter_references/zreferences.html#ioffe-szegedy-2015), **a popular and effective technique** that consistently **accelerates the convergence of deep nets**.

- **å¯¹è¾“å…¥çš„æ ‡å‡†åŒ–ï¼ˆæµ…å±‚æ¨¡å‹ï¼‰**

  å¤„ç†åçš„ä»»æ„ä¸€ä¸ªç‰¹å¾åœ¨æ•°æ®é›†ä¸­æ‰€æœ‰æ ·æœ¬ä¸Šçš„å‡å€¼ä¸º0ã€æ ‡å‡†å·®ä¸º1ã€‚æ ‡å‡†åŒ–å¤„ç†è¾“å…¥æ•°æ®ä½¿å„ä¸ªç‰¹å¾çš„åˆ†å¸ƒç›¸è¿‘
  
- **æ‰¹é‡å½’ä¸€åŒ–ï¼ˆæ·±åº¦æ¨¡å‹ï¼‰**

  åˆ©ç”¨å°æ‰¹é‡ä¸Šçš„å‡å€¼å’Œæ ‡å‡†å·®ï¼Œä¸æ–­è°ƒæ•´ç¥ç»ç½‘ç»œä¸­é—´è¾“å‡ºï¼Œä»è€Œä½¿æ•´ä¸ªç¥ç»ç½‘ç»œåœ¨å„å±‚çš„ä¸­é—´è¾“å‡ºçš„æ•°å€¼æ›´ç¨³å®šã€‚
  

#### å¯¹å…¨è¿æ¥å±‚åšæ‰¹é‡å½’ä¸€åŒ–

- ä½¿ç”¨å…¨è¿æ¥å±‚çš„æƒ…å†µï¼Œ**è®¡ç®—ç‰¹å¾ç»´ä¸Šçš„å‡å€¼å’Œæ–¹å·®**

- ä½ç½®ï¼š**å…¨è¿æ¥å±‚ä¸­çš„ä»¿å°„å˜æ¢å’Œæ¿€æ´»å‡½æ•°ä¹‹é—´**

  - **å…¨è¿æ¥ï¼š**
  $$
    \begin{aligned} \boldsymbol{x}=\boldsymbol{W} \boldsymbol{u}+\boldsymbol{b} \\ \text {output}=\phi(\boldsymbol{x}) \end{aligned}
  $$
  
  - **æ‰¹é‡å½’ä¸€åŒ–ï¼š**
  $$
  \begin{aligned} \text { output } &=\phi(\mathrm{BN}(\boldsymbol{x})) \\ \boldsymbol{y}^{(i)} &=\mathrm{BN}\left(\boldsymbol{x}^{(i)}\right) \\ \boldsymbol{\mu}_{\mathcal{B}} & \leftarrow \frac{1}{m} \sum_{i=1}^{m} \boldsymbol{x}^{(i)} \\ \boldsymbol{\sigma}_{\mathcal{B}}^{2} & \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(\boldsymbol{x}^{(i)}-\boldsymbol{\mu}_{\mathcal{B}}\right)^{2} \\ \hat{\boldsymbol{x}}^{(i)} & \leftarrow \frac{\boldsymbol{x}^{(i)}-\boldsymbol{\mu}_{\mathcal{B}}}{\sqrt{\boldsymbol{\sigma}_{\mathcal{B}}^{2}+\epsilon}} \end{aligned}
  $$
  
    è¿™â¾¥Ïµ > 0æ˜¯ä¸ªå¾ˆå°çš„å¸¸æ•°ï¼Œä¿è¯åˆ†æ¯å¤§äº0ï¼Œ$\boldsymbol{y}^{(i)} \leftarrow \boldsymbol{\gamma} \odot \hat{\boldsymbol{x}}^{(i)}+\boldsymbol{\beta}$ï¼Œå¼•å…¥**å¯å­¦ä¹ å‚æ•°**ï¼šæ‹‰ä¼¸å‚æ•°Î³å’Œåç§»å‚æ•°Î²ã€‚è‹¥ $\gamma=\sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}$ å’Œ $ \boldsymbol{\beta}=\boldsymbol{\mu}_{\mathcal{B}}$ æ‰¹é‡å½’ä¸€åŒ–æ— æ•ˆã€‚

#### å¯¹å·ç§¯å±‚åšæ‰¹é‡å½’â¼€åŒ–

- ä½¿ç”¨äºŒç»´å·ç§¯å±‚çš„æƒ…å†µï¼Œ**è®¡ç®—é€šé“ç»´ä¸Šï¼ˆaxis=1ï¼‰çš„å‡å€¼å’Œæ–¹å·®**
- ä½ç½®ï¼š**å·ç§¯è®¡ç®—ä¹‹åã€åº”â½¤æ¿€æ´»å‡½æ•°ä¹‹å‰**ã€‚
- å¦‚æœå·ç§¯è®¡ç®—è¾“å‡ºå¤šä¸ªé€šé“ï¼Œæˆ‘ä»¬éœ€è¦å¯¹è¿™äº›é€šé“çš„è¾“å‡º**åˆ†åˆ«åšæ‰¹é‡å½’ä¸€åŒ–ï¼Œä¸”æ¯ä¸ªé€šé“éƒ½æ‹¥æœ‰ç‹¬ç«‹çš„æ‹‰ä¼¸å’Œåç§»å‚æ•°ã€‚**
  - æ‹‰ä¼¸å‚æ•°å’Œåç§»å‚æ•°ä¸ºå¯å­¦ä¹ å‚æ•°ï¼Œ**éè¶…å‚æ•°**

#### é¢„æµ‹æ—¶çš„æ‰¹é‡å½’â¼€åŒ–

- **è®­ç»ƒ**ï¼šä»¥batchä¸ºå•ä½, å¯¹æ¯ä¸ªbatchè®¡ç®—å‡å€¼å’Œæ–¹å·®ã€‚
  **é¢„æµ‹**ï¼šç”¨ç§»åŠ¨å¹³å‡ä¼°ç®—æ•´ä¸ªè®­ç»ƒæ•°æ®é›†çš„æ ·æœ¬å‡å€¼å’Œæ–¹å·®ã€‚

```
net = nn.Sequential(
            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size
            BatchNorm(6, num_dims=4),
            nn.Sigmoid(),
            nn.MaxPool2d(2, 2), # kernel_size, stride
            nn.Conv2d(6, 16, 5),
            BatchNorm(16, num_dims=4),
            nn.Sigmoid(),
            nn.MaxPool2d(2, 2),
            d2l.FlattenLayer(),
            nn.Linear(16*4*4, 120),
            BatchNorm(120, num_dims=2),
            nn.Sigmoid(),
            nn.Linear(120, 84),
            BatchNorm(84, num_dims=2),
            nn.Sigmoid(),
            nn.Linear(84, 10)
        )
```

### Residual Networks (ResNet)

**æ·±åº¦å­¦ä¹ çš„é—®é¢˜**ï¼šæ·±åº¦CNNç½‘ç»œ**è¾¾åˆ°ä¸€å®šæ·±åº¦åå†ä¸€å‘³åœ°å¢åŠ å±‚æ•°**å¹¶ä¸èƒ½å¸¦æ¥è¿›ä¸€æ­¥åœ°åˆ†ç±»æ€§èƒ½æé«˜ï¼Œåè€Œä¼šæ‹›è‡´ç½‘ç»œæ”¶æ•›å˜å¾—æ›´æ…¢ï¼Œå‡†ç¡®ç‡ä¹Ÿå˜å¾—æ›´å·®ã€‚

- ResNeté€šè¿‡æ®‹å·®å­¦ä¹ è§£å†³äº†æ·±åº¦ç½‘ç»œçš„é€€åŒ–é—®é¢˜ï¼Œè®©æˆ‘ä»¬å¯ä»¥è®­ç»ƒå‡ºæ›´æ·±çš„ç½‘ç»œ

- **æ®‹å·®å—ï¼ˆResidual Blockï¼‰**

  <p align="center">
    <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/8uC02x.jpg" style="zoom:100%" />
  </p>

  **æ’ç­‰æ˜ å°„**ï¼š  

  å·¦è¾¹ï¼šf(x)=x                                                  

  å³è¾¹ï¼šf(x)-x=0 ï¼ˆæ˜“äºæ•æ‰æ’ç­‰æ˜ å°„çš„ç»†å¾®æ³¢åŠ¨ï¼‰

  åœ¨æ®‹å·®å—ä¸­ï¼Œ**è¾“â¼Šå¯é€šè¿‡è·¨å±‚çš„æ•°æ®çº¿è·¯æ›´å¿« åœ°å‘å‰ä¼ æ’­**ã€‚

#### ResNet Block

- **ResNet follows VGGâ€™s full 3Ã—3 convolutional layer design**. The residual block has **two 3Ã—3 convolutional layers** with the same number of output channels. **Each convolutional layer is followed by a batch normalization layer and a ReLU activation function**. Then, we skip these two convolution operations and add the input directly before the final ReLU activation function. This kind of design requires that the output of the two convolutional layers be of the same shape as the input, so that they can be added together. **If we want to change the number of channels or the stride, we need to introduce an additional 1Ã—1 convolutional layer to transform the input into the desired shape for the addition operation.**

  <p align="center">
    <img src="https://d2l.ai/_images/resnet-block.svg" style="zoom:100%" />
  </p>

```
class Residual(nn.Module):  # æœ¬ç±»å·²ä¿å­˜åœ¨d2lzh_pytorchåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
    #å¯ä»¥è®¾å®šè¾“å‡ºé€šé“æ•°ã€æ˜¯å¦ä½¿ç”¨é¢å¤–çš„1x1å·ç§¯å±‚æ¥ä¿®æ”¹é€šé“æ•°ä»¥åŠå·ç§¯å±‚çš„æ­¥å¹…ã€‚
    def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):
        super(Residual, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        if use_1x1conv:
            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.bn2 = nn.BatchNorm2d(out_channels)

    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        return F.relu(Y + X)
        
        
>>> blk = Residual(3, 3)
    X = torch.rand((4, 3, 6, 6))
    blk(X).shape # torch.Size([4, 3, 6, 6])
    
>>> blk = Residual(3, 6, use_1x1conv=True, stride=2)
		blk(X).shape # torch.Size([4, 6, 3, 3])
```

#### ResNet Model

- The first two layers of ResNet are the same as those of the GoogLeNet we described before: the 7Ã—77Ã—7 convolutional layer with 64 output channels and a stride of 2 is followed by the 3Ã—33Ã—3 maximum pooling layer with a stride of 2. The difference is the batch normalization layer added after each convolutional layer in ResNet.

  <p align="center">
    <img src="https://d2l.ai/_images/ResNetFull.svg" style="zoom:100%" />
  </p>

```
net = nn.Sequential(
        nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
        nn.BatchNorm2d(64), 
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
```

```
def resnet_block(in_channels, out_channels, num_residuals, first_block=False):
    if first_block:
        assert in_channels == out_channels # ç¬¬ä¸€ä¸ªæ¨¡å—çš„é€šé“æ•°åŒè¾“å…¥é€šé“æ•°ä¸€è‡´
    blk = []
    for i in range(num_residuals):
        if i == 0 and not first_block:
            blk.append(Residual(in_channels, out_channels, use_1x1conv=True, stride=2))
        else:
            blk.append(Residual(out_channels, out_channels))
    return nn.Sequential(*blk)

net.add_module("resnet_block1", resnet_block(64, 64, 2, first_block=True))
net.add_module("resnet_block2", resnet_block(64, 128, 2))
net.add_module("resnet_block3", resnet_block(128, 256, 2))
net.add_module("resnet_block4", resnet_block(256, 512, 2))

net.add_module("global_avg_pool", d2l.GlobalAvgPool2d()) # GlobalAvgPool2dçš„è¾“å‡º: (Batch, 512, 1, 1)
net.add_module("fc", nn.Sequential(d2l.FlattenLayer(), nn.Linear(512, 10))) 

>>> 0  output shape:	 torch.Size([1, 64, 112, 112])
    1  output shape:	 torch.Size([1, 64, 112, 112])
    2  output shape:	 torch.Size([1, 64, 112, 112])
    3  output shape:	 torch.Size([1, 64, 56, 56])
    resnet_block1  output shape:	 torch.Size([1, 64, 56, 56])
    resnet_block2  output shape:	 torch.Size([1, 128, 28, 28])
    resnet_block3  output shape:	 torch.Size([1, 256, 14, 14])
    resnet_block4  output shape:	 torch.Size([1, 512, 7, 7])
    global_avg_pool  output shape:	 torch.Size([1, 512, 1, 1])
    fc  output shape:	 torch.Size([1, 10])
```

### Densely Connected Networks (DenseNet)

- **The main difference between ResNet (left) and DenseNet (right) in cross-layer connections: use of addition and use of concatenation.**

  <p align="center">
    <img src="https://d2l.ai/_images/densenet-block.svg" style="zoom:100%" />
  </p>


#### **ä¸»è¦æ„å»ºæ¨¡å—**ï¼ˆç¨ å¯†å—ã€è¿‡æ¸¡å±‚ï¼‰

- **ç¨ å¯†å—**ï¼ˆdense blockï¼‰ï¼š å®šä¹‰äº†è¾“å…¥å’Œè¾“å‡ºæ˜¯å¦‚ä½•è¿ç»“çš„ã€‚

  **è¾“å‡ºé€šé“æ•°**=è¾“å…¥é€šé“æ•°+å·ç§¯å±‚ä¸ªæ•°*å·ç§¯è¾“å‡ºé€šé“æ•°

  ```
  def conv_block(in_channels, out_channels):
      blk = nn.Sequential(nn.BatchNorm2d(in_channels), 
                          nn.ReLU(),
                          nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))
      return blk
  
  class DenseBlock(nn.Module):
      def __init__(self, num_convs, in_channels, out_channels):
          super(DenseBlock, self).__init__()
          net = []
          for i in range(num_convs):
              in_c = in_channels + i * out_channels
              net.append(conv_block(in_c, out_channels))
          self.net = nn.ModuleList(net)
          self.out_channels = in_channels + num_convs * out_channels # è®¡ç®—è¾“å‡ºé€šé“æ•°
  
      def forward(self, X):
          for blk in self.net:
              Y = blk(X)
              X = torch.cat((X, Y), dim=1)  # åœ¨é€šé“ç»´ä¸Šå°†è¾“å…¥å’Œè¾“å‡ºè¿ç»“
          return X
          
  
  >>> blk = DenseBlock(2, 3, 10)
      X = torch.rand(4, 3, 8, 8)
      Y = blk(X)
      Y.shape # torch.Size([4, 23, 8, 8])
  ```

- **è¿‡æ¸¡å±‚**ï¼ˆtransition layerï¼‰ï¼šç”¨æ¥æ§åˆ¶é€šé“æ•°ï¼Œä½¿ä¹‹ä¸è¿‡å¤§ã€‚

  1Ã—1å·ç§¯å±‚ï¼š**æ¥å‡å°é€šé“æ•°**
  æ­¥å¹…ä¸º2çš„å¹³å‡æ± åŒ–å±‚ï¼š**å‡åŠé«˜å’Œå®½**

  ```
  def transition_block(in_channels, out_channels):
      blk = nn.Sequential(
              nn.BatchNorm2d(in_channels), 
              nn.ReLU(),
              nn.Conv2d(in_channels, out_channels, kernel_size=1),
              nn.AvgPool2d(kernel_size=2, stride=2))
      return blk
  
  >>> blk = transition_block(23, 10)
  		blk(Y).shape # torch.Size([4, 10, 4, 4])
  ```

  

### ä¼˜åŒ–ä¸æ·±åº¦å­¦ä¹ 

#### ä¼˜åŒ–ä¸ä¼°è®¡

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/V4e3jX.jpg" style="zoom:60%" />
</p>

- ä¼˜åŒ–æ–¹æ³•ç›®æ ‡ï¼š**è®­ç»ƒé›†æŸå¤±å‡½æ•°å€¼**

- æ·±åº¦å­¦ä¹ ç›®æ ‡ï¼š**æµ‹è¯•é›†æŸå¤±å‡½æ•°å€¼ï¼ˆæ³›åŒ–æ€§ï¼‰**

- **ä¼˜åŒ–åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„æŒ‘æˆ˜**

  - **å±€éƒ¨æœ€å°å€¼**

    <p align="center">
      <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/px7XYv.jpg" style="zoom:60%" />
    </p>

  - **éç‚¹**

    - **éç‚¹æ˜¯å¯¹æ‰€æœ‰è‡ªå˜é‡ä¸€é˜¶åå¯¼æ•°éƒ½ä¸º0ï¼Œä¸”HessiançŸ©é˜µç‰¹å¾å€¼æœ‰æ­£æœ‰è´Ÿçš„ç‚¹**
  
    <p align="center">
    <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/hVEBLK.jpg" style="zoom:60%" />
    </p>
  
    <p align="center">
    <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/tiY3KU.jpg" style="zoom:60%" />
    </p>

  - **æ¢¯åº¦æ¶ˆå¤±**
  
    <p align="center">
      <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/CVdhX9.jpg" style="zoom:60%" />
    </p>

#### å‡¸æ€§ ï¼ˆConvexityï¼‰

- ### **Sets**

  - Sets are the basis of convexity. Simply put, a set $ğ‘‹$ in a vector space is convex if for any $ğ‘,ğ‘âˆˆğ‘‹$ **the line segment connecting $ğ‘$ and $ğ‘$ is also in **$ğ‘‹$. In mathematical terms this means that for all $ğœ†âˆˆ[0,1]$ we have $\lambda \cdot a+(1-\lambda) \cdot b \in X$ whenever $a, b \in X$

  - **Three shapes, the left one is nonconvex, the others are convex** [Â¶](https://d2l.ai/chapter_optimization/convexity.html#id3)

    <p align="center">
      <img src="https://d2l.ai/_images/pacman.svg" style="zoom:60%" />
    </p>

  - **The intersection between two convex sets is convex** 

    <p align="center">
      <img src="https://d2l.ai/_images/convex-intersect.svg" style="zoom:60%" />
    </p>

  - **The union of two convex sets need not be convex**

    <p align="center">
      <img src="https://d2l.ai/_images/nonconvex.svg" style="zoom:60%" />
    </p>

- ### Functions
  - Given a convex set $ğ‘‹$ a function defined on it $ğ‘“:ğ‘‹â†’â„$ is convex if for all $ğ‘¥,ğ‘¥â€²âˆˆğ‘‹$ and for all $ğœ†âˆˆ[0,1]$ we have $\lambda f(x)+(1-\lambda) f\left(x^{\prime}\right) \geq f\left(\lambda x+(1-\lambda) x^{\prime}\right)$

  - As expected, the cosine function is nonconvex, whereas the parabola and the exponential function are.

    <p align="center">
      <img src="https://d2l.ai/_images/output_convexity_6bcbdc_3_0.svg" style="zoom:60%" />
    </p>

- ### Jensenâ€™s Inequality

  - **the expectation of a convex function is larger than the convex function of an expectation**

    $$
    \sum_{i} \alpha_{i} f\left(x_{i}\right) \geq f\left(\sum_{i} \alpha_{i} x_{i}\right) \text { and } E_{x}[f(x)] \quad \geq f\left(E_{x}[x]\right)
    $$
    
  - One of the **common applications of Jensenâ€™s inequality** is with regard to the **log-likelihood** of partially observed random variables.
    
    $$
    E_{y \sim P(y)}[-\log P(x | y)] \geq-\log P(x)
    $$
  
  

- ### Properties of Convexity

  - **æ— å±€éƒ¨æå°å€¼**

    - åä¾‹ï¼ˆå­˜åœ¨æ¯”å±€éƒ¨æœ€å°å€¼æ›´å°çš„å€¼ï¼‰

      <p align="center">
        <img src="https://d2l.ai/_images/output_convexity_6bcbdc_5_0.svg" style="zoom:100%" />
      </p>

  - **ä¸å‡¸é›†çš„å…³ç³»**

    - åä¾‹ï¼ˆéå‡¸å‡½æ•°çš„å¯¹åº”çš„é›†åˆä¹Ÿæ˜¯éå‡¸çš„ï¼‰

      <p align="center">
        <img src="https://d2l.ai/_images/output_convexity_6bcbdc_7_0.svg" style="zoom:100%" />
      </p>

  - **äºŒé˜¶æ¡ä»¶**

    $f^{''}(x) \ge 0 \Longleftrightarrow f(x)$ æ˜¯å‡¸å‡½æ•°

    **å¿…è¦æ€§ ($\Leftarrow$):**

    å¯¹äºå‡¸å‡½æ•°ï¼š

    $$
    \frac{1}{2} f(x+\epsilon)+\frac{1}{2} f(x-\epsilon) \geq f\left(\frac{x+\epsilon}{2}+\frac{x-\epsilon}{2}\right)=f(x)
    $$

    æ•…:

    $$
    f^{\prime \prime}(x)=\lim _{\varepsilon \rightarrow 0} \frac{\frac{f(x+\epsilon) - f(x)}{\epsilon}-\frac{f(x) - f(x-\epsilon)}{\epsilon}}{\epsilon}
    $$

    $$
    f^{\prime \prime}(x)=\lim _{\varepsilon \rightarrow 0} \frac{f(x+\epsilon)+f(x-\epsilon)-2 f(x)}{\epsilon^{2}} \geq 0
    $$

    **å……åˆ†æ€§ ($\Rightarrow$):**
    
    ä»¤ $a < x < b$ ä¸º $f(x)$ ä¸Šçš„ä¸‰ä¸ªç‚¹ï¼Œç”±æ‹‰æ ¼æœ—æ—¥ä¸­å€¼å®šç†:
    
    $$
    \begin{array}{l}{f(x)-f(a)=(x-a) f^{\prime}(\alpha) \text { for some } \alpha \in[a, x] \text { and }} \\ {f(b)-f(x)=(b-x) f^{\prime}(\beta) \text { for some } \beta \in[x, b]}\end{array}
    $$

    æ ¹æ®å•è°ƒæ€§ï¼Œæœ‰ $f^{\prime}(\beta) \geq f^{\prime}(\alpha)$, æ•…:
    
    $$
    \begin{aligned} f(b)-f(a) &=f(b)-f(x)+f(x)-f(a) \\ &=(b-x) f^{\prime}(\beta)+(x-a) f^{\prime}(\alpha) \\ & \geq(b-a) f^{\prime}(\alpha) \end{aligned}
    $$

- ### Constraints

  - One of the nice properties of convex optimization is that it allows us to handle constraints efficiently. That is, it allows us to solve problems of the form: 
    
    $$
    \begin{array}{l}{\underset{\mathbf{x}}{\operatorname{minimize}} f(\mathbf{x})} \\ {\text { subject to } c_{i}(\mathbf{x}) \leq 0 \text { for all } i \in\{1, \ldots, N\}}\end{array}
    $$
    
    Here $ğ‘“$ is the objective and the functions $ğ‘_ğ‘–$ are constraint functions. To see what this does consider the case where $ğ‘_1(ğ±)=â€–ğ±â€–_2âˆ’1$. In this case the parameters **ğ±** are constrained to the unit ball. If a second constraint is $c_2(x)=v^âŠ¤x + b$, then this corresponds to all **ğ±** lying on a halfspace. Satisfying both constraints simultaneously amounts to selecting a slice of a ball as the constraint set.
    
  - **å¯ä»¥å¼•å…¥æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•æ±‚è§£**
  
    $$
    L(\mathbf{x}, \alpha)=f(\mathbf{x})+\sum_{i} \alpha_{i} c_{i}(\mathbf{x}) \text { where } \alpha_{i} \geq 0
    $$
    
    

### æ¢¯åº¦ä¸‹é™

#### ä¸€ç»´æ¢¯åº¦ä¸‹é™

**è¯æ˜ï¼šæ²¿æ¢¯åº¦åæ–¹å‘ç§»åŠ¨è‡ªå˜é‡å¯ä»¥å‡å°å‡½æ•°å€¼**

æ³°å‹’å±•å¼€ï¼š

$$
f(x+\epsilon)=f(x)+\epsilon f^{\prime}(x)+\mathcal{O}\left(\epsilon^{2}\right)
$$

ä»£å…¥æ²¿æ¢¯åº¦æ–¹å‘çš„ç§»åŠ¨é‡ $\eta f^{\prime}(x)$ï¼š

$$
f\left(x-\eta f^{\prime}(x)\right)=f(x)-\eta f^{\prime 2}(x)+\mathcal{O}\left(\eta^{2} f^{\prime 2}(x)\right)
$$

$$
f\left(x-\eta f^{\prime}(x)\right) \lesssim f(x)
$$

$$
x \leftarrow x-\eta f^{\prime}(x)
$$

#### å¤šç»´æ¢¯åº¦ä¸‹é™

$$
\nabla f(\mathbf{x})=\left[\frac{\partial f(\mathbf{x})}{\partial x_{1}}, \frac{\partial f(\mathbf{x})}{\partial x_{2}}, \dots, \frac{\partial f(\mathbf{x})}{\partial x_{d}}\right]^{\top}
$$

$$
f(\mathbf{x}+\epsilon)=f(\mathbf{x})+\epsilon^{\top} \nabla f(\mathbf{x})+\mathcal{O}\left(\|\epsilon\|^{2}\right)
$$

$$
\mathbf{x} \leftarrow \mathbf{x}-\eta \nabla f(\mathbf{x})
$$

<p align="center">
  <img src="https://d2l.ai/_images/output_gd_9cd2d2_15_1.svg" style="zoom:100%" />
</p>

#### è‡ªé€‚åº”æ–¹æ³•

- **ç‰›é¡¿æ³•**

  - åœ¨ $x + \epsilon$ å¤„æ³°å‹’å±•å¼€ï¼š

    $$
    f(\mathbf{x}+\epsilon)=f(\mathbf{x})+\epsilon^{\top} \nabla f(\mathbf{x})+\frac{1}{2} \epsilon^{\top} \nabla \nabla^{\top} f(\mathbf{x}) \epsilon+\mathcal{O}\left(\|\epsilon\|^{3}\right)
    $$

    æœ€å°å€¼ç‚¹å¤„æ»¡è¶³: $\nabla f(\mathbf{x})=0$, å³æˆ‘ä»¬å¸Œæœ› $\nabla f(\mathbf{x} + \epsilon)=0$, å¯¹ä¸Šå¼å…³äº $\epsilon$ æ±‚å¯¼ï¼Œå¿½ç•¥é«˜é˜¶æ— ç©·å°ï¼Œæœ‰ï¼š

    $$
    \nabla f(\mathbf{x})+\boldsymbol{H}_{f} \boldsymbol{\epsilon}=0 \text { and hence } \epsilon=-\boldsymbol{H}_{f}^{-1} \nabla f(\mathbf{x})
    $$
    

- **Preconditioning**

  - Quite unsurprisingly computing and storing **the full Hessian is very expensive**. It is thus desirable to find **alternatives**. One way to improve matters is by **avoiding to compute the Hessian** in its entirety but **only compute the *diagonal* entries**. While this is not quite as good as the full Newton method, it is still much better than not using it. Moreover, estimates for the main diagonal elements are what drives some of the innovation in stochastic gradient descent optimization algorithms. This leads to update algorithms of the form

    $$
    \mathbf{x} \leftarrow \mathbf{x}-\eta \operatorname{diag}\left(H_{f}\right)^{-1} \nabla \mathbf{x}
    $$
    
#### Stochastic Gradient Descent

- **å‚æ•°æ›´æ–°**

  - å¯¹äºæœ‰ $n$ ä¸ªæ ·æœ¬å¯¹è®­ç»ƒæ•°æ®é›†ï¼Œè®¾ $f_i(x)$ æ˜¯ç¬¬ $i$ ä¸ªæ ·æœ¬çš„æŸå¤±å‡½æ•°, åˆ™ç›®æ ‡å‡½æ•°ä¸º:

    $$
    f(\mathbf{x})=\frac{1}{n} \sum_{i=1}^{n} f_{i}(\mathbf{x})
    $$

    å…¶æ¢¯åº¦ä¸º:

    $$
    \nabla f(\mathbf{x})=\frac{1}{n} \sum_{i=1}^{n} \nabla f_{i}(\mathbf{x})
    $$

    ä½¿ç”¨è¯¥æ¢¯åº¦çš„ä¸€æ¬¡æ›´æ–°çš„æ—¶é—´å¤æ‚åº¦ä¸º $\mathcal{O}(n)$

    éšæœºæ¢¯åº¦ä¸‹é™æ›´æ–°å…¬å¼ $\mathcal{O}(1)$:

    $$
    \mathbf{x} \leftarrow \mathbf{x}-\eta \nabla f_{i}(\mathbf{x})
    $$

    ä¸”æœ‰ï¼š

    $$
    \mathbb{E}_{i} \nabla f_{i}(\mathbf{x})=\frac{1}{n} \sum_{i=1}^{n} \nabla f_{i}(\mathbf{x})=\nabla f(\mathbf{x})
    $$

<p align="center">
  <img src="https://d2l.ai/_images/output_sgd_0425f7_3_1.svg" style="zoom:100%" />
</p>

```
def sgd(params, states, hyperparams):
    for p in params:
        p.data -= hyperparams['lr'] * p.grad.data
        

def train_ch7(optimizer_fn, states, hyperparams, features, labels,
              batch_size=10, num_epochs=2):
    # åˆå§‹åŒ–æ¨¡å‹
    net, loss = d2l.linreg, d2l.squared_loss
    
    w = torch.nn.Parameter(torch.tensor(np.random.normal(0, 0.01, size=(features.shape[1], 1)), dtype=torch.float32), requires_grad=True)
    b = torch.nn.Parameter(torch.zeros(1, dtype=torch.float32), requires_grad=True)

    def eval_loss():
        return loss(net(features, w, b), labels).mean().item()

    ls = [eval_loss()]
    data_iter = torch.utils.data.DataLoader(
        torch.utils.data.TensorDataset(features, labels), batch_size, shuffle=True)
    
    for _ in range(num_epochs):
        start = time.time()
        for batch_i, (X, y) in enumerate(data_iter):
            l = loss(net(X, w, b), y).mean()  # ä½¿ç”¨å¹³å‡æŸå¤±
            
            # æ¢¯åº¦æ¸…é›¶
            if w.grad is not None:
                w.grad.data.zero_()
                b.grad.data.zero_()
                
            l.backward()
            optimizer_fn([w, b], states, hyperparams)  # è¿­ä»£æ¨¡å‹å‚æ•°
            if (batch_i + 1) * batch_size % 100 == 0:
                ls.append(eval_loss())  # æ¯100ä¸ªæ ·æœ¬è®°å½•ä¸‹å½“å‰è®­ç»ƒè¯¯å·®
    # æ‰“å°ç»“æœå’Œä½œå›¾
    print('loss: %f, %f sec per epoch' % (ls[-1], time.time() - start))
    d2l.set_figsize()
    d2l.plt.plot(np.linspace(0, num_epochs, len(ls)), ls)
    d2l.plt.xlabel('epoch')
    d2l.plt.ylabel('loss')
```



