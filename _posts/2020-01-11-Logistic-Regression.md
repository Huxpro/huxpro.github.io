---
layout:     post
title:      Logistic Regression
subtitle:   é€»è¾‘å›å½’
date:       2020-01-11
author:     Young
header-img: img/bg-post/0*xuDhGc5E9EVQdbQE.png
catalog: true
tags:
    - machine learning
    - python
---

### [Linear Regression vs Logistic Regression](https://www.javatpoint.com/linear-regression-vs-logistic-regression-in-machine-learning)

<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/pKFgxq.jpg" style="zoom:80%" />
</p>

- **Main difference between them is how they are being used**
  <br>
  The Linear Regression is used for **solving Regression problems** whereas Logistic Regression is used for **solving the Classification problems**. 

  <table class="alt">
  <tbody><tr>
  	<th>Linear Regression</th>
  	<th>Logistic Regression</th>
  </tr>
  <tr>
    <td>Linear regression is used to predict the <b>continuous dependent variable</b> using a given set of independent variables.</td>
    <td>Logistic Regression is used to predict the <b>categorical dependent variable </b> using a given set of independent variables.</td>
  </tr>
  <tr>
    <td>Linear Regression is used for <b>solving Regression problem</b>.</td>
    <td>Logistic regression is used for <b>solving Classification problems</b>.</td>
  </tr>
  <tr>
    <td>In Linear regression, we predict the value of <b>continuous variables</b>.</td>
    <td>In logistic Regression, we predict the values of <b>categorical variables</b>.</td>
  </tr>
  <tr>
    <td>In linear regression, we find <b>the best fit line</b>, by which we can easily predict the output.</td>
    <td>In Logistic Regression, we find <b>the S-curve</b> by which we can classify the samples.</td>
  </tr>
  <tr>
    <td><b>Least square estimation method</b> is used for estimation of accuracy.</td>
    <td><b>Maximum likelihood estimation method</b> is used for estimation of accuracy.</td>
  </tr>
  <tr>
    <td>The output for Linear Regression must be a <b>continuous value</b>, such as price, age, etc.</td>
  	<td>The output of Logistic Regression must be a <b>Categorical value</b> such as 0 or 1, Yes or No, etc.</td>
  </tr>
  <tr>
  	<td>In Linear regression, it is required that relationship between dependent variable and independent variable must be linear.</td>
  	<td>In Logistic regression, it is not required to have the linear relationship between the dependent and independent variable.</td>
  </tr>
  <tr>
  	<td>In linear regression, there may be collinearity between the independent variables.</td>
  	<td>In logistic regression, there should not be collinearity between the independent variable.</td>
  </tr>
  </tbody></table>

### Theory

- **Decision boundary**
  <p align="center">
    <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/1bLM2Q.jpg" style="zoom:80%" />
  </p>
  
  - Suppose we have a generic training set $\lbrace \left(x^{(1)}, y^{(1)}\right),\left(x^{(2)}, y^{(2)}\right), \ldots,\left(x^{(m)}, y^{(m)}\right) \rbrace$, where $ğ‘¥(ğ‘š)$  is the input variable of the ğ‘š-th example, while $ğ‘¦(ğ‘š)$ is its output variable, ranging from 0 to 1. Finally we have the hypothesis function for logistic regression, $h_{\theta}(x)=\frac{1}{1+e^{-\theta^T x}}$.

### Interpretation

**[The cost function used in linear regression won't work here](https://www.internalpointers.com/post/cost-function-logistic-regression)**

- If we try to use the cost function of the linear regression in Logistic Regression $\sum^m_{i=1}(y^{(i)}-\frac{1}{1+e^{-\theta^T x}})^2$, then it would be of no use as it would **end up being a non-convex function with many local minimums**, in which it would be very **difficult to minimize the cost value and find the global minimum**.
  <p align="center">
    <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/LQSAIt.jpg/" style="zoom:80%" />
  </p>

- Logistic regression cost function
  <p align="center">
  $$
  cost\left(h_{\theta}(x), y\right)=\left\{\begin{array}{ll}{-\log \left(h_{\theta}  
  (x)\right)} & {\text { if } y=1} \\ {-\log \left(1-h_{\theta}(x)\right)} & {\text { if } y=0}\end{array}\right.
  $$
  </p>
  å³ï¼Œ$cost\left(h_{\theta}(x), y\right)=-y \log \left(h_{\theta}(x)\right)-(1-y) \log \left(1-h_{\theta}(x)\right)$
  
- With the optimization in place, the logistic regression cost function can be rewritten as:
  <p align="center">
  $$
  \begin{aligned} J(\theta) &=\frac{1}{m} \sum_{i=1}^{m} 
  cost\left(h_{\theta}\left(x^{(i)}\right), y^{(i)}\right) \\ &=-\frac{1}{m}\left[\sum_{i=1}^{m} y^{(i)} \log    
  \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]   \end{aligned}
  $$
  </p>

- **$\frac{\partial}{\partial \theta_{j}} J(\theta)=\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}$[æ¨å¯¼è¿‡ç¨‹](https://stats.stackexchange.com/questions/278771/how-is-the-cost-function-from-logistic-regression-derivated)**
<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/BYYvPc.jpg" style="zoom:80%" />
</p>

- **$\frac{d}{d x} \sigma(x)=\sigma(x)(1-\sigma(x))$[æ¨å¯¼è¿‡ç¨‹](https://stats.stackexchange.com/questions/278771/how-is-the-cost-function-from-logistic-regression-derivated)**
<p align="center">
  <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/j0MRaf.jpg" style="zoom:80%" />
</p>

### [é€»è¾‘å›å½’çš„åˆ†å¸ƒå¼å®ç°](https://blog.csdn.net/qq_32742009/article/details/81839071)

- **æŒ‰è¡Œå¹¶è¡Œ**
  <br>
  **å³å°†æ ·æœ¬æ‹†åˆ†åˆ°ä¸åŒçš„æœºå™¨ä¸Šå»ï¼ŒæŠŠæ•°æ®é›†æ‰“æ•£**ï¼ˆæ³¨ï¼šå³â€œåˆ’åˆ†â€ï¼Œè¦æ»¡è¶³ä¸é‡ã€ä¸æ¼ä¸¤ä¸ªæ¡ä»¶ï¼‰æˆ C å—ï¼Œ$\frac{\partial J}{\partial w}=\frac{1}{N} \sum_{k=1}^{K} \sum_{i \in C_{k}}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \times x^{(i)}$ï¼Œ ç„¶åå°†è¿™ C å—åˆ†é…åˆ°ä¸åŒçš„æœºå™¨ä¸Šå»ï¼Œåˆ™åˆ†å¸ƒå¼çš„è®¡ç®—æ¢¯åº¦ï¼Œåªä¸è¿‡æ˜¯**æ¯å°æœºå™¨éƒ½è®¡ç®—å‡ºå„è‡ªçš„æ¢¯åº¦ï¼Œç„¶åå½’å¹¶æ±‚å’Œå†æ±‚å…¶å¹³å‡**ã€‚
  - **ä¸ºä»€ä¹ˆå¯ä»¥è¿™ä¹ˆåšå‘¢ï¼Ÿ**
  <br>
  **æ¢¯åº¦ä¸‹é™å…¬å¼åªä¸ä¸Šä¸€ä¸ªæ›´æ–°æ‰¹æ¬¡çš„$\theta$åŠå½“å‰æ ·æœ¬æœ‰å…³**

- **æŒ‰åˆ—å¹¶è¡Œ**
  <br>
  æŒ‰åˆ—å¹¶è¡Œçš„æ„æ€å°±æ˜¯**å°†åŒä¸€æ ·æœ¬çš„ç‰¹å¾ä¹Ÿåˆ†å¸ƒåˆ°ä¸åŒçš„æœºå™¨ä¸­å»**ã€‚ä¸Šé¢çš„å…¬å¼ä¸ºé’ˆå¯¹æ•´ä¸ª$\theta$ï¼Œå¦‚æœæˆ‘ä»¬åªæ˜¯é’ˆå¯¹æŸä¸ªåˆ†é‡$\theta_{j}$ï¼Œå¯å¾—åˆ°å¯¹åº”çš„æ¢¯åº¦è®¡ç®—å…¬å¼ $ \frac{\partial J}{\partial w}=\frac{1}{N} \sum_{i=1}^{N}\left(h_{\theta_j}\left(x_{j}^{(i)}\right)-y^{(i)}\right) \times x_{j}^{(i)} $ ï¼Œå³ä¸å†æ˜¯ä¹˜ä»¥æ•´ä¸ª $x_n$ï¼Œè€Œæ˜¯ä¹˜ä»¥ $x_n$ å¯¹åº”çš„åˆ†é‡ $x_{n,j}$ï¼Œæ­¤æ—¶å¯ä»¥å‘ç°ï¼Œ**æ¢¯åº¦è®¡ç®—å…¬å¼ä»…ä¸$x_n$ä¸­çš„ç‰¹å¾æœ‰å…³ç³»**ï¼Œæˆ‘ä»¬å°±å¯ä»¥**å°†ç‰¹å¾åˆ†å¸ƒåˆ°ä¸åŒçš„è®¡ç®—ä¸Šï¼Œåˆ†åˆ«è®¡ç®—$\theta_{j}$å¯¹åº”çš„æ¢¯åº¦ï¼Œæœ€åå½’å¹¶ä¸ºæ•´ä½“çš„$\theta$ï¼Œå†æŒ‰è¡Œå½’å¹¶åˆ°æ•´ä½“çš„æ¢¯åº¦æ›´æ–°ã€‚**

### [é€»è¾‘å›å½’çš„ä¼˜ç¼ºç‚¹](http://theprofessionalspoint.blogspot.com/2019/03/advantages-and-disadvantages-of.html)

- Advantages of Logistic Regression
  - $1.$ Logistic Regression **performs well when the dataset is linearly separable**.ï¼ˆ**å¯¹äºéçº¿æ€§çš„æ•°æ®é›†é€‚åº”æ€§å¼±**ï¼‰
  - $2.$ Logistic regression is less prone to over-fitting but it can overfit in high dimensional datasets. You should consider Regularization (L1 and L2) techniques to avoid over-fitting in these scenarios.
  - $3.$ Logistic Regression not only gives a measure of how relevant a predictor (coefficient size) is, but also its direction of association (positive or negative).
  - $4.$ Logistic regression is **easier to implement, interpret and very efficient to train**. 

- Disadvantages of Logistic Regression
  - $1.$ Main limitation of Logistic Regression is **the assumption of linearity** between the dependent variable and the independent variables. In the real world, the data is rarely linearly separable. Most of the time data would be a jumbled mess.
  - $2.$ If the number of observations are lesser than the number of features, Logistic Regression should not be used, otherwise it may lead to overfit.ï¼ˆ**å½“ç‰¹å¾ç©ºé—´å¾ˆå¤§ï¼Œæ€§èƒ½æ¬ ä½³**ï¼‰
  - $3.$ Logistic Regression can only be **used to predict discrete functions**. Therefore, the dependent variable of Logistic Regression is restricted to the discrete number set. This restriction itself is problematic, as it is prohibitive to the prediction of continuous data.
  - $4.$ å› ä¸ºé¢„æµ‹ç»“æœå‘ˆZå­—å‹ï¼ˆæˆ–åZå­—å‹ï¼‰ï¼Œå› æ­¤**å½“æ•°æ®é›†ä¸­åœ¨ä¸­é—´åŒºåŸŸæ—¶ï¼Œå¯¹æ¦‚ç‡çš„å˜åŒ–ä¼šå¾ˆæ•æ„Ÿï¼Œå¯èƒ½ä½¿å¾—é¢„æµ‹ç»“æœç¼ºä¹åŒºåˆ†åº¦**ã€‚ 

### [Handling Imbalanced Classes In Logistic Regression](https://chrisalbon.com/machine_learning/logistic_regression/handling_imbalanced_classes_in_logistic_regression/)

- Like many other learning algorithms in scikit-learn, LogisticRegression comes with a **built-in method of handling imbalanced classes**. If we have highly imbalanced classes and have no addressed it during preprocessing, we have the option of using the **class_weight parameter** to weight the classes to make certain we have a balanced mix of each class. Specifically, the balanced argument will automatically weigh classes inversely proportional to their frequency: $w_{j}=\frac{n}{k n_{j}}$, **where $w_j$ is the weight to class $j$, $n$ is the number of observations, $n_j$ is the number of observations in class $j$, and $k$ is the total number of classes**.

### [sklearn.linear_model.LogisticRegressionéƒ¨åˆ†å‚æ•°è§£æ](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)

Parameters            | detail
:-------------------------:|:-------------------------:
penalty: {â€˜l1â€™, â€˜l2â€™, â€˜elasticnetâ€™, â€˜noneâ€™}, default=â€™l2â€™| 
dual: bool, default=False|Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features.
tolfloat:  default=1e-4|è¿­ä»£ç»ˆæ­¢åˆ¤æ–­çš„è¯¯å·®èŒƒå›´
C: float, default=1.0|å…¶å€¼ç­‰äºæ­£åˆ™åŒ–å¼ºåº¦çš„å€’æ•°ï¼Œä¸ºæ­£çš„æµ®ç‚¹æ•°ã€‚æ•°å€¼è¶Šå°è¡¨ç¤ºæ­£åˆ™åŒ–è¶Šå¼ºã€‚
fit_intercept: bool, default=True|æŒ‡å®šæ˜¯å¦åº”è¯¥å‘å†³ç­–å‡½æ•°æ·»åŠ å¸¸é‡(å³åå·®æˆ–æˆªè·)
intercept_scaling: float, default=1|ä»…ä»…å½“solveræ˜¯â€liblinearâ€æ—¶æœ‰ç”¨
class_weight: dict or â€˜balancedâ€™, default=None|è°ƒæ•´æ ·æœ¬ä¸å‡è¡¡é—®é¢˜

### é€»è¾‘å›å½’+ç‰›é¡¿æ³•+æ¢¯åº¦ä¸‹é™è®²ä¹‰è¡¥å……


<p align="center">
    <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/wYMvHe.jpg" style="zoom:80%" />
</p>
<p align="center">
    <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Ef30sB.jpg" style="zoom:80%" />
</p>
<p align="center">
    <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/Kckl7a.jpg" style="zoom:80%" />
</p>
<p align="center">
    <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/N9t7yl.jpg" style="zoom:80%" />
</p>
<p align="center">
    <img src="https://gitee.com/echisenyang/GiteeForUpicUse/raw/master/uPic/QotN3P.jpg" style="zoom:80%" />
</p>

### [pythonå®ç°(ä»£ç é“¾æ¥)](https://gitee.com/echisenyang/GiteeForFileUse/blob/master/ipynb/Task3_logistic_regression.ipynb)
- 1ã€å…ˆå°è¯•è°ƒç”¨sklearnçš„çº¿æ€§å›å½’æ¨¡å‹è®­ç»ƒæ•°æ®ï¼Œå°è¯•ä»¥ä¸‹ä»£ç ï¼Œç”»å›¾æŸ¥çœ‹åˆ†ç±»çš„ç»“æœ
- 2ã€ç”¨æ¢¯åº¦ä¸‹é™æ³•å°†ç›¸åŒçš„æ•°æ®åˆ†ç±»ï¼Œç”»å›¾å’Œsklearnçš„ç»“æœç›¸æ¯”è¾ƒ
- 3ã€ç”¨ç‰›é¡¿æ³•å®ç°ç»“æœï¼Œç”»å›¾å’Œsklearnçš„ç»“æœç›¸æ¯”è¾ƒï¼Œå¹¶æ¯”è¾ƒç‰›é¡¿æ³•å’Œæ¢¯åº¦ä¸‹é™æ³•è¿­ä»£æ”¶æ•›çš„æ¬¡æ•°
