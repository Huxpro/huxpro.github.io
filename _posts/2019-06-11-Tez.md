# TEZ

Tez的设计是基于Yarn的，下图为Yarn的架构

![MapReduce NextGen Architecture](https://hadoop.apache.org/docs/r3.1.3/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif)

上图中涉及到自定义的类有

* Client对应的是TezClient
* AppMaster对应的是DAGAppMaster
* Container对应的是TezChild

 

Tez源码版本为0.2.0



### Processor如何被调用的

![tez-0.2.0-processor](.\文档图片\tez-0.2.0-processor.png)

### Task之间的数据是如何传递的，即Edge是如何处理的

在定义edge时其实就已经指定了task的输入数据类，该类用于处理数据的读取和写入，在LogicalIOProcessorRuntimeTask中会用到相应的reader和writer。在原先的MR中task间的数据传输只有shuffle阶段，但在tez可以通过定义edge实现不同的传输逻辑。

### Vertex和Task是什么关系

Vertex可以并行，类似于MR任务中的多mapper，而每个执行的进程是一个task

### Task是线程还是进程

Task是进程，也就是每一个task对应于一个TezChild，这个和MR中的mapTask/reduceTask类似。TezChild-->TezTaskRunner2-->LogicalIOProcessorRuntimeTask-->执行Processor逻辑

### Tez如何实现MR的shuffle-sort

### VertexManager的作用

1. 控制Task的并发度
2. 确定vertex中task何时开始

### DagSchedular的作用

1. 确定task优先级

### TaskSchedular的作用

1. 从yarn中申请容器并提交task



### 难点

通过消息实现组件之间的解耦，导致组件之间的调用分析较为困难。

### 技巧

由于最新的代码较为庞大，所以选择最原始的代码阅读，代码只有最新的20%，能降低很多复杂度。

### Tez的优势

#### 自动并发reduce

map阶段的task会发送统计数据到reduce的vertexManager，确定正确的并行度。

[How does ShuffleVertexManager (Auto Reduce Parallelism) work](https://cwiki.apache.org/confluence/display/TEZ/How+does+ShuffleVertexManager+%28Auto+Reduce+Parallelism%29+work)



注意一个限制`ShuffleVertexManager mainly works on scatter-gather sources`，在非该情况时，不会有这样的优化。




#### 消除中间数据缓存

例如`select * from (select * from a group by name) as p1 join (select * from b group by age) as p2 `，该sql在mr中执行必须先生存中间数据才能进行join，但是在tez中没有这样的问题。同时也不存在多阶段提交的问题，降低了任务执行调度时间。

MR的执行步骤：

1. Read data from file -->one disk access

2. Run mappers

3. Write map output --> second disk access

4. Run shuffle and sort --> read map output, third disk access

5. write shuffle and sort --> write sorted data for reducers --> fourth disk access

6. Run reducers which reads sorted data --> fifth disk output

7. Write reducers output -->sixth disk access

 example：

```sql
select g1.x, g1.avg, g2.cnt
from (select a.x, average(a.y) as avg from a group by a.x) g1
         join (select b.x, count(b.y) as avg from b group by b.x) g2
              on (g1.x = g2.x)
order by avg;
```

![SQL_MR](.\文档图片\SQL_MR.png)

![SQL_MR](.\文档图片\SQL_TEZ.png)

主要的优势在于，对于tez中的task没有阶段区分，但在mr中无法在reduce阶段读取数据，只有map阶段可以。

#### Reudce阶段提前执行

map阶段的task会发送统计数据到reduce的vertexManager，当reduce阶段不需要全部数据时，也就是说不需要map阶段完成。那么就可以体检执行reduce任务（shuffle阶段提前开始）。



### 问题

#### 如何指定yarn中运行的java程序jdk版本

由于创建appMaster和childJVM命令中，使用的jvm启动参数都是获取环境变量后创建的(MR和TEZ都是如此)，所以无法单独制定。

#### Tez中JDK版本的要求

在0.8.5版本之后都都需要JDK1.8

#### Tez安装

只需将tez的tar包放到集群中，并且在tez-site.xml中配置其路径即可。原理，当通过tez-client向yarn提交任务时，会将指定的包作为其classpath的一部分（解压工作由yarn完成），而mr中不需要上次mr运行的包，是因为其代码中直接使用了HADOOP_CLASSPATH作为其classpath

#### Tez Session Mode

当客户端设置为session模式时，那么会使用相同的`DAGAppMaster`提交任务（使用rpc通信），否则的话，由yarn创建`DAGAppMaster`

#### 关于Counter的限制

无论是在mapReduce还是在Tez中都限制了counter的个数，前者默认是120个后者默认是1200个，需要注意在mapReduce中通过Limit更改其个数是无效的，这是个buag，目前唯一的方式是修改配置文件，在tez中修复了这个bug。

那么为什么需要限制counter的个数呢？

首先每个task的counter统计信息是通过taskReporter的心跳发送到ApplicationMaster的，这不是问题所在。

在tez中一个dag任务的层级是dag--vertex--task--taskAttempt；当想要获取任务的统计信息时就需要一个层级一个层级的合并，每个层级都可能存在多个（dag只有一个），同时这些个信息都是存在appMaster，如果使用tez的session功能，那么可能会导致由于counter过多而出现OOM，如果不使用的话，基本没什么问题，该参数调整的大些也是ok的。

在MR中的限制，是由于在1.x版本中，所有的统计信息都是用JobTracker（有且只有一个，这也是yarn出现的原因）保存，所以一旦counter过多就极易出现OOM。

#### GroupSplit的作用

在TEZ-534中有说明，主要是为了优化输入切分，原本的mr切分方式不是很完善

#### 关于 Split data	

tez有2中split方式，一是在client进行，二是在Am中进行，使用二方式更加高效

#### HIVE LLAP

hive基于tez实现的一个特性`Live Long and Process`，文档链接：[LLAP](https://cwiki.apache.org/confluence/display/Hive/LLAP)

注意和OLAP的名词区别，OLAP表示Online analytical processing



### 参考文章

1. [PowerPoint Presentation](https://docs.huihoo.com/infoq/qconshanghai-apache-tez-next-generation-execution-engine-on-hadoop-20141016.pdf)
2. [Hive + Tez: A Performance Deep Dive](https://www.slideshare.net/Hadoop_Summit/w-235phall1pandey)
3. [[Hive on Tez](https://cwiki.apache.org/confluence/display/Hive/Hive+on+Tez)](https://cwiki.apache.org/confluence/display/Hive/Hive+on+Tez)
